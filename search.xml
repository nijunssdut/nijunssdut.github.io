<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[集成学习专题（二）]]></title>
    <url>%2Fpassages%2F2019-07-30-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%93%E9%A2%98%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[GBDTGBDT是集成学习Boosting的一种。Gradient Boosting的主要的思想是，每一次建立单个学习器时，是在之前建立的模型的损失函数的梯度下降方向。损失函数越大，说明模型越容易出错，如果我们的模型能够让损失函数持续下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度的方向上下降。 GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。所以为了得到残差，GBDT中的树都是回归树，不是分类树。 GBDT的算法流程输入是训练集样本，最大迭代次数T，每轮迭代输入数据是训练集无放回采样样本，损失函数L。 初始化弱学习器。 $$T(x;\theta_m)$$ T表示决策树，x为输入样本，$\theta_m$为树分裂参数。 对迭代轮数m = 1,2,…,T有： （a）计算各个叶子区域损失函数L的负梯度值，将它作为残差的估计$$r_{mi} = -[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]{f(x)=f{m-1}(x)}$$（b）对$r_{mi}$拟合为一颗新的回归树，根据新的回归树得到m轮产生的叶子节点区域 （c）遍历回归树所有叶子节点区域，在各个区域使损失函数极小化找到残差 （d）更新强学习器 得到输出的最终模型 $$f_M(x) = \sum_{m =1}^M T(x;\theta_m)$$ GBDT常用的损失函数GBDT中为什么用负梯度来拟合残差计算其实除了均方误差的情况一阶导是残差外，其他的情况没有残差的概念，GBDT每一轮拟合的都是损失函数负梯度。 使用梯度计算代替的主要原因是为了将GBDT扩展到更复杂的损失函数中。 当损失函数形式简单，可以认为$y^{‘}（模型输出值）= y（实际值）$时损失函数最小，但当损失函数加入了正则项后，并非$y^{‘}=y$时损失函数取得最小值，所以我们需要计算损失函数的梯度，而不能直接使用模型来计算残差。 GBDT不适合使用高维稀疏特征的原因 特征太多，GBDT不一定跑的动，即使可以跑也会耗费时间，因为在每一次分割时需要比较大量的特征。 树的分割往往只考虑了少量特征，大部分特征用不到，少量的特征在多次分裂时被重复用到，剩余的长尾基本用不到，所有高维稀疏特征会造成大量特征的浪费。 GBDT减少误差的方式机器学习算法的误差分为偏差和方差两个部分。 GBDT迭代每一步都在拟合当前模型预测值和真实值之间的偏差，通过不断的迭代使偏差减小，所以只要选取方差较小的模型作为基分类器，GBDT就可以很好的减小预测误差。 GBDT的优缺点GBDT主要的优点有： （1）可以灵活处理各种类型的数据，包括连续值和离散值。 （2）在相对少的调参时间情况下，预测的准确率也可以比较高 （3）使用一些健壮的损失函数，对异常值的鲁棒性非常强，比如Huber损失函数和Quantile损失函数 GBDT的主要缺点有： （1）由于弱学习器之间存在依赖关系，难以并行训练数据。 GBDT如何进行正则化第一种正则化方式为步长（learning rate）。定义为v，对于前面的弱学习器的迭代$$f_k{x} = f_{k-1}(x)+h_k(x)$$如果我们加上了正则化项，则有$$f_k(x) = f_{k-1}(x)+ vh_k(x)$$v的取值范围为$0\lt v \le 1$。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数，通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5,0.8]之间。 第三种是对于弱学习器即CART回归树进行正则化剪枝。 GBDT如何构建特征GBDT如何用于分类GBDT需要调试的参数GBDT训练中需要调试的参数如下： n_estimators：也就是弱学习器的最大迭代次数，或者说最大弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，容易过拟合，一般选择一个适中的数值。 learning_rate：即每个弱学习器的权重缩减系数v，也称步长。 subsample：不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。 max_features允许单个决策树使用特征的最大数量。 max_depth决策树最大深度 默认决策树在建立子树的时候不会限制子树的深度 min_sample_split 内部节点再划分所需最小样本数 内部节点再划分所需最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 min_samples_leaf 叶子节点最少样本数 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 max_leaf_nodes 最大叶子节点数 通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内的最优的决策树。 min_impurity_split 节点划分最小不纯度 这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>GBDT</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习专题（一）]]></title>
    <url>%2Fpassages%2F2019-07-30-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%93%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[集成学习算法严格意义上来说，集成学习算法不算是一种机器学习算法，而更像是一种优化手段或者策略，它通常是结合多个简单的弱机器学习算法，去做更可靠的决策。 集成方法是由多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及如何结合的方法。 集成学习主要三种框架集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。 Bagging：基于数据随机重抽样的分类器构建方法。从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。 Boosting：训练过程为阶梯状，基模型按次序一一进行训练（实际上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，每次都是提高前一次分错了的数据集的权值，最后对所有基模型预测的结果进行线性组合产生最终的预测结果。 Stacking：将训练好的所有基模型对训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个特征值，最后基于新的训练集进行训练。同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。 Bagging算法流程输入为样本集D={(x1,y1),(x2,x2),…,(xm,ym)}，弱学习器算法，弱分类器迭代次数T 输出为最终的强分类器f(x) （1）对于t = 1,2,…,T： 对训练集进行第t次随机采样，共采集T次，得到包含T个样本的采样集$D_t$ 用采样集$D_t$训练第t个弱学习器$G_t(x)$ （2）如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 常用bagging算法：随机森林算法 Boosting算法流程 给定初始训练数据，由此训练出第一个基学习器； 根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注； 用调整后的样本，训练下一个基学习器； 重复上述过程T次，将T个学习器加权结合。 $$f(x) = w_0 + \sum_{m = 1}^Mw_m\Psi_m(x)$$ 其中w是权重，$\Psi$是弱分类器的集合，可以看出最终就是基函数的线形组合。 常用boosting算法：Adaboost，GBDT，XGBoost stacking的使用stacking常见的使用方式：由KNN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的Logistic回归组合。 随机森林随机森林是Bagging算法的代表，它的核心思想就是将多个不同的决策树进行组合，利用这种组合降低单一决策树有可能带来的片面性和判断不准确性。 随机森林使用了CART决策树作为弱学习器，并对决策树的建立做了改进，对于普通的决策树，会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是随机森林通过随机选择节点上的一部分样本特征，这个数字小于n，假设为nsub，然后在这些随机选择的nsub个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强模型的泛化能力。 如果nsub=n，则此时随机森林的CART决策树和普通的CART决策树没有区别。nsub越小，则模型越健壮，当然此时对于训练集的拟合程度会变差。也就是说nsub越小，模型的方差会减小，但偏差会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的nsub的值。 算法流程输入为样本集D ={(x1,y1),(x2,y2),…,(xm,ym)}，弱分类器迭代次数T。 输出为最终的强分类器f(x) (1) 对于t=1,2,…,T： 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_t$ 用采样集$D_t$训练第t个决策树模型$G_t(x)$，在训练决策树模型的节点的时候，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分 (2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 随机森林的随机性 随机森林的随机性体现在每棵树的训练样本是随机的。 随机森林的树中每个节点的分裂属性集合也是随机选择确定的。 随机森林不易过拟合原因随机森林由很多棵树组合在一起，单看每一颗树都可以是过拟合的，但是，既然是过拟合，就会拟合到非常小的细节上，因此随机森林通过引入随机性，让每一颗树拟合的细节不同，这时再把这些树组合在一起，过拟合的部分就会自动被消除掉。 所以随机森林不容易过拟合，但这不是绝对的，随机森林也是有可能出现过拟合的现象，只是出现的概率相对低。 如何使用随机森林去弥补特征向量中的缺失值对于训练集中的缺失值，可以使用均值，0等方式进行预填充，然后使用随机森林分类，同一个分类下的数据，更新缺失值，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补，然后再次使用随机森林分类更新缺失值，4-6轮后可以达到一个比较好的效果。 随机森林对特征重要性的评估首先要了解随机森林的袋外数据（oob）误差的计算方法。 随机森林的袋外数据指每次随机抽取未被抽到的数据。 袋外数据（oob）误差的计算方式如下： 对于已经生成的随机森林，用袋外数据测试其性能，假设袋外数据总数为O，用这O个袋外数据作为输入，带进之前已经生成的随机森林分类器，分类器会给出O个数据相应的分类，因为这O条数据的类型是已知的，则用正确的分类与随机森林分类器的结果进行比较，统计随机森林分类器分类错误的数目，设为X，则袋外数据误差大小 = X/O；这已经经过证明是无偏估计的，所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。 在随机森林中某个特征X的重要性的计算方法如下： 对于随机森林中的每一颗决策树，使用相应的OOB（袋外数据）来计算它的袋外数据误差，记为$errOOB_1$。 随机地对袋外数据OOB所有样本的特征X加入噪声干扰（可以随机的改变样本在特征X处的值），再次计算它的袋外数据误差，记为$errOOB_2$。 假设随机森林中有Ntree棵树，那么对于特征X的重要性=$\sum(errOOB_2-errOOB_1)/Ntree$，之所以可以用这个表达式来作为相应特征的重要性的度量是因为：若给某个特征随机加入噪声后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。 随机森林训练需要调整的参数随机森林中主要需要调整参数： n_estimators 随机森林建立子树的数量。 较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量。 max_feature随机森林允许单个决策树使用特征的最大数量。 增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。但是，可以肯定，通过增加max_features会降低算法的速度。因此，你需要适当的平衡和选择最佳max_features。 max_depth 决策树最大深度 默认决策树在建立子树的时候不会限制子树的深度。 min_samples_split 内部节点再划分所需最小样本数 内部节点再划分所需最小样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 min_samples_leaf 叶子节点最少样本数 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 max_leaf_nodes 最大叶子节点数 通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。 min_impurity_split 节点划分最小不纯度 这个值限制了决策树的增长，如果某节点的不纯度（基于基尼系数，均方差）小于这个阈值，则该节点不再生成子节点。即为叶子节点。一般不推荐改动默认值1e-7。 随机森林不用全样本训练m棵决策树全样本训练忽视了局部样本的规律（各个决策树趋于相同），对于模型的泛化能力是有害的，使随机森林算法在样本层面失去了随机性。 随机森林算法优缺点优点： （1）训练可以高度并行化，对于大数据时代的大样本训练速度有优势。 （2）随机森林对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出变量的重要性程度，这是一个非常便利的功能。 （3） 在对缺失数据进行评估时，随机森林是一个十分有效的方法。就算存在大量的数据缺失，随机森林也能较好地保持精确性，一方面因为随机森林随机选取样本和特征，另一方面因为它可以继承决策树对缺失数据的处理方式。 （4）由于采用了随机采样，训练出的模型的方差小，泛化能力强。 （5）当存在分类不平衡时，随机森林是能够提供平衡数据集误差的有效方法。 缺点： （1）随机森林对回归问题的解决并没有像它在分类中表现那么好。它并不能给出一个连续的输出，而且不能够做出超出训练集数据范围的预测，当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致某些特定噪声的数据进行建模时出现过度拟合，随机森林已经被证明在某些噪音较大的分类或者回归问题上会过拟合。 （2） 对于很多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试，可能有很多相似的决策树，掩盖了真实的结果。 （3） 对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。 AdaBoostAdaBoost分类器就是一种元算法分类器，Adaboost分类器利用同一种基分类器（弱分类器），基于分类器的错误率分配不同的权重参数，最后累加加权的预测结果作为输出。 AdaBoost算法流程Adaboost算法流程如下： （1） 给数据中的每一个样本一个权重，权重的起始值是相同的，若有n个训练样本，其权重均为1/n； （2）训练数据中的每一个样本，得到第一个分类器； （3）计算该分类器的错误率，根据错误率计算要给分类器分配的权重（这里是分类器的权重，不是样本的权重）$$错误率\varepsilon = \sum_{i \in 错分类样本}W_i, W_i为样本权重$$ $$分类器权重\alpha = \frac{1}{2}In(\frac{1-\varepsilon}{\varepsilon})$$ （4）将第一个分类器分错误的样本权重增加，分对的样本权重减小（这里是样本的权重，不是分类器的权重）$$错误样本权重更新公式D_i^{t+1} = \frac{D_i^te^{\alpha}}{sum(D_i^t)}$$ $$正确样本权重更新公式D_i^{t+1} = \frac{D_i^te^{-\alpha}}{sum(D_i^t)}$$ t指当前分类器，i指第i个样本 （5）然后再用新的样本权重训练数据，得到新的分类器，到步骤3 （6）直到步骤3中分类器错误率为0或者整体弱分类器为0，或者到达迭代次数 （7）将所有弱分类器加权求和，得到分类结果（注意是分类器权重），错误率低的分类器获得更高的决定系数，从而在对数据进行预测时起关键作用。$$f(x) = \sum\alpha h_i(x),h_i(x)为各个弱分类器$$ AdaBoost优缺点优点： Adaboost提供一种框架，在框架内可以使用各种方法构建子分类器。可以使用简单的弱分类器，不用对特征进行筛选，也不存在过拟合的现象。 Adaboost算法不需要弱分类器的先验知识，最后得到的强分类器的分类精度依赖于所有弱分类器。无论是应用于人造数据还是真实数据，Adaboost都能显著的提高学习精度。 Adaboost算法不需要预先知道弱分类器的错误率上限，且最后得到的强分类器的分类精度依赖于所有弱分类器的分类精度，可以深挖分类器的能力。 Adaboost可以根据弱分类器的反馈，自适应地调整假定的错误率，执行的效率高。 Adaboost对同一个训练样本集训练不同的弱分类器，按照一定的方法把这些弱分类器集合起来，构造一个分类能力很强的强分类器。 缺点： 在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。 Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。 AdaBoost对噪声敏感的原因在Adaboost训练过程中，Adaboost会使得难于分类样本权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。 AdaBoost与随机森林的异同随机森林和Adaboost算法都可以用来分类，它们都是优秀的基于决策树的组合算法。 这两种分类方法的相同之处： 二者都是bootstrap自助法选取样本。 二者都要训练很多棵决策树。 两种分类方法的不同之处： Adaboost是基于boosting的算法，随机森林是基于bagging的算法。 Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。 随机森林在训练每一棵树的时候，随机挑选了部分特征作为拆分特征，而不是所有的特征都去作为拆分特征。 在观测新数据时，adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
        <tag>随机森林</tag>
        <tag>AdaBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K邻近（KNN）专题]]></title>
    <url>%2Fpassages%2F2019-07-29-K%E9%82%BB%E8%BF%91%EF%BC%88KNN%EF%BC%89%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[KNN算法原理KNN算法又称为k最近邻分类算法。所谓的k最近邻，就是指最接近的k个邻居（数据），即每个样本都可以由它的k个邻居来表达。 KNN算法的核心思想是，在一个含未知样本的空间，可以根据离这个样本最邻近的k个样本的数据类型来确定样本的数据类型。 该算法涉及3个主要因素：分类决策规则、距离与相似的衡量、k的大小。 KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的k个样本，预测为里面有最多类别数的类别。而KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。 对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离。 k值的选择，过小则容易过拟合，过大则容易欠拟合，可以使用交叉验证法选取k值。 KNN算法优缺点优点： 思想简单，理论成熟，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 准确度高，对数据没有假设，对离群值不敏感。 缺点： 计算量大； 样本不平衡问题（即有些类别的样本数量很多，而其他样本的数量很少）； 需要大量的内存。 KNN中不平衡样本问题的解决方案KNN在分类时重要的不足在于当样本不平衡时（即一个类的样本容量很大，而其他类样本数量很小），很可能导致当输入一个未知样本时，该样本的k个邻居中大数量类的样本占多数。但是这类样本并不接近目标样本，而数量小的这类样本很靠近目标样本。这个时候，我们有理由认为该位置样本属于数量小的样本所属的一类，但是，KNN却不关心这个问题，它只关心哪类样本的数量最多，而不去把距离远近考虑在内，因此，会导致预测结果的不准确。 我们可以采用权值的方法来改进。和样本距离小的邻居权值大，和样本距离大的邻居权值则相对较小，由此，将距离远近的因素也考虑在内，避免因一个样本过大导致误判的情况。 KNN算法计算量过大的解决方案KNN算法计算量较大，因为对每一个待分类的样本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。KNN算法的改进方法之一是分组快速搜素近邻法。其基本思想是：将样本集按近邻关系分解成组，给出每组质心的位置，以质心作为代表点，和未知样本计算距离，选出距离最近的一个或若干个组，再在组的范围内应用一般的KNN算法。由于并不是将未知样本与所有样本计算距离，故该改进算法可以减少计算量，但不能不减少存储量。 KD树的概念kd树（k-dimension tree）是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是一种二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。这种空间划分就是对数据点进行分类，“挨得近”的数据点就在一个空间里面。利用kd树可以省去对大部分数据点的搜索，从而减少搜素的计算量。 kd树的构造构造根结点，使得根结点对应于k维空间中包含所有实例点的超矩形区域；通过下面的递归的方法，不断地对k维空间进行切分，生成子结点。在超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分成左右两个子区域（子结点）；这时，实例被分到两个子区域（子结点）；这时，实例被分到两个子区域，这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的。 构造平衡kd树算法输入：k维空间数据集$T={x_1,x_2,…,x_N}$，其中$x_i ={x_i(1),x_i(2),…,x_i(k)}，i=1,2,…,N$； 输出：kd树 （1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。选择x(1)为坐标轴，以T中所有实例的x(1)坐标中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x(1)垂直的超平面实现。由根结点生成深度为1的左、右子结点：左子结点对应坐标x(1)小于切分点的子区域，右子结点对应于坐标x(1)大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。 （2）重复：对深度为j的结点，选择x(I)为切分的坐标轴，I =j%k+1，以该结点的区域中所有实例的x(I)坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x(I)垂直的超平面实现。由该结点生成深度为j+1的左、右子结点：左子结点对应坐标x(I)小于切分点的子区域，右子结点对应坐标x(I)大于切分点的子区域。将落在切分超平面上的实例点保存在该结点。 搜索kd树利用kd树可以省去对大部分数据点的搜素，从而减少搜索的计算量。 用kd树的最近邻搜索： 输入：已构造的kd树；目标点x； 输出：x的最近邻 （1）在kd树中找出包含目标点x的叶结点：从根结点出发，递归的向下访问kd树。若目标点当前维的坐标值小于切分点的坐标值，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止； （2）以此叶结点为“当前最近点”； （3）递归的向上回退，在每个结点进行以下操作： 如果该结点保存的实例点比当前最近点距目标点更近，则以该实例点为“当前最近点”； 当前最近点一定存在于该结点一个子结点对应的区域是否有更近的点。具体的，检查另一个子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子结点对应的区域内存在距离目标更近的点，移动到另一个子结点。接着，递归的进行最近邻搜索。如果不相交，向上回退。 （4） 当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。该方法可以修改后用于k近邻搜索，即通过维护一个包含k个最近邻结点的队列来实现。 优化kd树建立过程中切分维度的顺序构建开始前，对比数据点在各维度的分布情况，数据点在某一维度坐标值的方差越大分布越分散，方差越小分布越集中。从方差大的维度开始切分可以取得很好的切分效果及平衡性。 kd树每一次继续切分都要计算该子区间在需切分维度上的中值，计算量很大，如何优化？ 算法开始前，对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。 从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。 k-Means与KNN的区别 KNN是分类算法，k-Means是聚类算法； KNN是监督学习，k-Means是非监督学习； KNN给它的数据集是带label的数据，已经是完全正确的数据，k-Means给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序； KNN没有明显的前期训练过程，k-Means有明显的前期训练过程； k的含义 KNN是对一个样本x给它分类，即求出它的y。就是从数据集中，在x附近找离它最近的k个数据点，这k个数据点，类别c占的个数最多，就把x的label设为c。 k-Means中k是人工固定好的数字，假设数据集合可以分为k个簇，由于是依靠人工定好，需要一点先验知识。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>有监督学习</tag>
        <tag>kNN</tag>
        <tag>分类回归算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树专题]]></title>
    <url>%2Fpassages%2F2019-07-27-%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[决策树的构建过程构建步骤如下： 将所有的特征看成一个一个的节点 遍历每个特征的每一种分割方式，找到最好的分割点；将数据划分为不同的子节点，eg:N1，N2…；计算划分之后所有子节点的“纯度”信息； 对第二步产生的分割，选择出最优的特征以及最优的划分方式；得出最终的子节点：N1，N2…Nm； 对子节点N1、N2…Nm分别继续执行2-3步，直到每个最终的子节点都足够‘纯’。 决策树的原理决策树是一类常见的机器学习方法，它是基于树的结构进行决策的，决策过程通常会进行一系列的判断或者子决策。 每次做决策时根据哪一个属性呢，即如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一个类别，即节点的“纯度（purity）”越来越高。 决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别调用此方法，返回结点，返回的结点就是上一层的子结点。直到所有特征都已经用完，或者数据集只有一维特征为止。 信息熵假设当前样本集合D中有n类样本${x_1,x_2,……,x_n}$，第i类样本在样本集合D中所占的比例为$p(x_i)$，则熵定义为信息的期望值，$x_i$的信息定义为：$$l(x_i)= -\log_2p(x_i)$$而样本集合D的信息熵定义为：$$Ent(D) = -\sum_{i=1}^np(x_i)\log_2p(x_i)$$$Ent(D)$的值越小，则D的纯度越高。 信息增益假定离散属性a有k个可能的取值{a1,a2,……,ak}，若使用a来对样本集D进行划分，则会产生k个分支节点，其中第i个分支节点包含了D中所有在属性a上取值ai的样本，记为$D_i$，我们可根据公式求得第i个分支节点的信息熵记为$Ent(D_i)$，第i个分支节点样本集$D_i$占样本集合D总的样本比例记为$p(D_i) = D_i/D$，父节点的信息熵记为$Ent(D)$，则用属性a对样本D进行划分所获得的“信息增益”为：$$Gain(D,a)= Ent(D)-\sum_{i=1}^kp(D_i)Ent(D_i)$$一般而言，信息增益越大，表现使用属性a来进行划分所获得的“纯度提升”越大。 信息增益率如果使用信息增益去对决策树进行划分，可能出现每个分支节点只包含一个样本，这些分支节点的纯度已经达到最大，但是，这样的决策树显然不具备泛化能力，无法对新样本进行有效的预测。 实际上，信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，因此提出了信息增益率的概念：$$Gain_ration(D,a) = Gain(D,a)/IV(a)\IV(a) = -\sum_{i =1}^kp(D_i)\log_2(D_i)$$属性$a$的可能取值数目越多，则IV(a)的值通常会越大。 基尼指数基尼指数定义：$$Gini(D) = \sum_{k=1}^Kp_k(1-p_k)$$基尼指数反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小，则数据集D的纯度越高，属性a的基尼指数定义为：$$Gini_index(D,a) = \sum_{i=1}^kp(D_i)Gini(D_i)$$于是在候选集合中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。 常用的决策树算法常用的决策树算法有ID3，C4.5和CART。 ID3：以信息增益为准则来选择划分属性，多叉树。 C4.5：使用信息增益率来进行属性的选择，多叉树。 CART：使用基尼指数来选择划分属性，二叉树。 C4.5对ID3做了哪些改进ID3算法是采用信息增益作为评价标准进行分支的决策树算法。 ID3的缺点： 对于具有很多值的属性它是非常敏感的，例如，如果我们数据集中的某个属性值对不同的样本基本上是不相同的，甚至更极端点，对于每个样本都是唯一的，如果我们用这个属性来划分数据集，它会得到很大的信息增益，但是，这样的结果并不是我们想要的。 ID3算法不能处理具有连续值的属性。 ID3算法不能处理属性具有缺失值的样本。 由于按照该算法会生成很深的树，所以容易产生过拟合现象。 C4.5算法主要对ID3做出了以下方面的改进。 用信息增益率来选择属性，克服了用信息增益来选择属性时偏向选择值多的属性的不足。 可以处理连续数值型属性。 缺失值处理：对于某些采样数据，可能会缺少属性值。在这种情况下，处理缺少属性值的通常做法是赋予该属性的常见值，或者属性均值。另外一种比较好的方法是为该属性的每个可能值赋予一个概率，即将该属性以概率形式赋值。这样处理的目的是计算信息增益，使得这种属性值缺失的样本也能处理。 C4.5的缺点： 算法低效，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。 内存受限，适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。 C4.5算法处理连续数值型属性对于连续分布的特征，其处理方法是： 先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$&lt;=v_j$的分到左子树，$&gt;v_j$的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性发生改变的地方（即分类发生了变化）才需要切开，这可以显著减少运算量。经证明，在决定连续特征的分界点时采用增益这个指标，而选择属性的时候才使用增益率这个指标能选择出最佳分类特征。 对连续属性的处理如下： 对特征的取值进行升序排序； 两个特征取值之间的中点作为可能的分裂点，将数据集分为两部分，计算每个可能的分裂点的信息增益。优化算法就是只计算分类属性发生改变的那些特征取值； 选择修正后信息增益最大的分裂点作为该特征的最佳分裂点； 计算最佳分裂点的信息增益率作为特征的信息增益率。注意，此处需对最佳分裂点的信息增益进行修正：减去$\log_2(N-1)/|D|$（N是连续特征的取值个数，D是训练数据数目，此修正的原因在于：当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点） C4.5与CART的区别两者都是决策树，但CART既可以做分类，又可以做回归，而C4.5只能用于分类。 C4.5是构造决策树来发现数据中蕴涵的分类规则，是一种通过划分特征空间逼近离散函数值的方法。C4.5是基于ID3的改进算法，使用信息增益率作为划分依据。分类规则是互斥且完备的，所谓互斥即每一条样本记录不会同时匹配上两条分类规则，所谓完备即每条样本记录都在决策树中都能匹配上一条规则。 CART本质是对特征空间进行二元划分（即CART生成的决策树是一棵二叉树），并能够对标量属性与连续属性进行分裂。在对标量进行划分时，分为等于该属性和不等于该属性；对连续进行划分时，分为大于和小于。并且在分类的时候是采用GINI作为衡量标准，而不是信息增益了；而在回归时，是使用均方误差作为评价。 CART对于特征的利用是可以重复的，而作为分类的C4.5则是不能重复利用特征的。 CART树对离散特征取值数目&gt;=3的特征如何处理因为CART树是二叉树，所以对于样本的有N&gt;=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点。 如某特征值具有[“young”,”middle”,”old”]三个取值，那么二分序列会有如下3种可能性（空集和满集在CART分类中没有意义）: [((‘young’,),(‘middle’,’old’)),((‘middle’,),(‘young’,’old’)),((‘old’,),(‘young’,’middle’))] 采用CART算法，就需要分别计算按照上述List中的二分序列做分叉时的Gini指数，然后选取产生最小的GiniGain的二分序列做该特征的分叉二值序列参与树构建的递归。 因此CART不适用于离散特征有过多个取值可能的场景。此时，若定要使用CART，则最好预先人为的将离散特征的取值缩减。 那么对于二分后的左右分支，如果特征取值tuple中的元素多于2个，该特征还可以继续参与当前子数据集的二分。 决策树的剪枝决策树剪枝方法预剪枝通过提前停止树的构建而对树剪枝，一旦停止，节点就是树叶，该树叶表示子集包含最频繁的类。 常用剪枝条件： 定义一个高度，当决策树达到给高度时就停止决策树的生长； 定义一个阈值，当达到某个节点的实例个数小于阈值时就可以停止决策树的生长； 定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值大小来决定是否停止决策树的生长。 缺点：这样做决策树无法到最优，也无法得到比较好的效果。 后剪枝它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。 错误率降低剪枝 悲观错误剪枝 代价复杂度剪枝 缺点：这种方法比较浪费前面的建立过程（计算上） 决策树需要剪枝的原因决策树是充分考虑了所有的数据点而生成的复杂树，有可能出现过拟合的情况，决策树越复杂，过拟合的程度会越来越高。 考虑极端的情况，如果我们令所有的叶子节点都只含有一个数据点，那么我们能够保证所有的训练数据都能准确分类，但是很有可能得到高的预测误差，原因是将训练数据中所有的噪声数据都“准确划分”了，强化了噪声数据的作用。（形成决策树的目的是作出合理的预测，尽可能有效的排除噪声数据干扰，影响正确预测的判断） 剪枝修剪分裂前后分类误差相差不大的子树，能够降低决策树的复杂度，降低过拟合出现的概率。 决策树特征分类树和回归树分类树以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的阈值，按照该标准分枝得到两个新节点，用同样方法继续分枝得到两个新节点，用同样方法继续分枝直到得到种类唯一的叶子节点，或达到预设的终止条件，若最终叶子节点的种类不唯一，则以占有最多数的种类作为该叶子节点的标识。 回归树回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即（每个人的年龄-预测年龄）^ 2的总和/N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 决策树在选择特征进行分类时一个特征被选择过后，之后还会选择到这个特征吗？决策树中一个特征被选择过后依然是有可能被选择为分裂特征的。 若特征为离散特征，如果决策树为二叉树，则可以在分类的子区间继续划分，如果决策树为多叉树，通常进行一次划分。 若特征为连续特征，则可能在决策树中多次被选择。 常用的决策树一定是二叉树？二叉决策树与多分支决策树相比有什么特点？决策树并非均为二叉树，CART算法要求其所生成的决策树为二叉树，而ID3和C4.5则允许决策树为多分支的。 二叉决策树不像多叉树那样会形成过多的数据碎片，而二叉决策树可能会得到更深的最终决策树。 决策树需要进行归一化处理吗？概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，决策树是一种概率模型，数值缩放，不影响分裂点位置。所以一般可以不对其进行归一化处理。 一棵决策树构建过程耗时的步骤是？决策树的构建最耗时的步骤是确定最佳分割点，在该步骤中需要对特征的值进行排序，这个过程时间损耗较多。 如果决策树属性用完了仍未对决策树完成划分应该怎么办？在决策树构造过程中可能会出现这种情况：所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点。 存在时间序列回归模型效果比决策树模型有更高的精度的原因时间序列数据有线性关系，而决策树算法是已知的检测非线性交互最好的算法。为什么决策树没能提供好的预测的原因是它不能像回归模型一样做到对线性关系那么好的映射。因此，我们知道了如果我们有一个满足线性回归模型能提供强大的预测。 决策树的优缺点优点： 易于理解，决策树易于理解和实现。人们在通过解释后都有能力去理解决策树所表达的意义。 数据处理简单，对于决策树，数据的准备往往是简单或者是不必要的。其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。 是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。 易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。 缺点： 很容易在训练数据中生成复杂的树结构，造成过拟合(overfitting)。剪枝可以缓解过拟合的负作用，常用方法是限制树的高度、叶子节点中的最少样本数量。 不适合处理高维数据，当属性数量过大的时候，部分决策树就不太适用了。 对异常值(Outlier)过于敏感，很容易导致树的结构的巨大的变换。 泛化(Generalization)能力太差，对于没有出现过的值几乎没有办法。 决策树对缺失值的处理决策树缺失值需要考虑3个问题： 当开始决定选择哪个属性用来进行分支时，如果有些训练样本缺失了某些属性值时该怎么办？ 忽略这些缺失属性a的样本。 填充缺失值，例如给缺失属性a的样本赋予属性a一个均值或者最常用的值或者根据其他未知的属性想办法把这些样本缺失的属性补全。 计算信息增益或者信息增益率时根据缺失属性样本个数所占的比率对增益/增益率进行相应的“打折”，例如我们对10个样本进行分类，一个样本数据在a属性具有缺失值，我们使用其中九个完全样本计算a属性的信息增益，并将其乘以0.9作为最终信息增益。 一个属性已被选择，那么在决定分支的时候如果有些样本缺失了该属性如何处理？ 忽略这些样本 填充缺失值再进行分类，例如给缺失属性a的样本赋予属性a一个均值或者最常用的值者根据其他未知的属性想办法把这些样本缺失的属性补全。 把这些属性缺失样本，按照具有属性a的样本被划分成的子集样本个数的相对比率，分配到各个子集中去。至于哪些缺失的样本被划分到子集1，哪些被划分到子集2，这个没有一定的准则，可以随机而动。 把属性缺失样本分配给所有的子集，也就是说每个子集都有这些属性缺失样本。 单独为属性缺失的样本划分一个分支子集。 当决策树已经生成，但待分类的样本缺失了某些属性，这些属性该如何处理？ 如果有单独的缺失值分支，依据此分支预测。 把待分类的样本的属性a值分配一个最常出现的a的属性值，然后进行分支预测。 根据其他属性为该待分类样本填充一个属性a值，然后进行分支处理。 待分类样本在到达属性a节点时就终止分类，然后根据此时a节点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>ID3</tag>
        <tag>C4.5</tag>
        <tag>CART</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马尔可夫模型]]></title>
    <url>%2Fpassages%2F2019-07-26-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[知识背景马尔可夫链马尔可夫过程是满足无后效性的随机过程。假设一个随机过程中，$t_n$时刻的状态$x_n$的条件分布，仅仅与其前一个状态$x_{n-1}$有关，即$P(x_n|x_1,x_2,…x_{n-1})=P(x_n|x_{n-1})$，则将其称为马尔可夫过程，时间和状态的取值都是离散的马尔可夫过程也称为马尔可夫链。 隐马尔可夫模型（HMM）隐马尔可夫模型是对含有未知参数(隐状态)的马尔可夫链进行建模的生成模型，概率图模型如下图所示。 ![Hidden Markov Model](2019-07-26-马尔可夫模型/Hidden Markov Model.png) 在简单的马尔可夫模型中，所有状态对于观测者都是可见的，因此在马尔可夫模型中仅仅包括状态间的转移概率。而在隐马尔可夫模型中，隐状态$x_i$对于观测者而言是不可见的，观测者能观测到的只有每个隐状态$x_i$对应的输出$o_i$，而观测状态$o_i$的概率分布仅仅取决于对应的隐状态$x_i$。在隐马尔可夫模型中，参数包含了隐状态间的转移概率、隐状态到观测状态的输出概率、隐状态$x$的取值空间、观测状态$o$的取值空间以及初始状态的概率分布。 隐马尔可夫模型三个基本问题与相应的算法前向、后向算法解决的是评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。（概率计算问题） Baum-Welch算法解决的是模型训练问题，求解使得该观测序列概率最大的模型参数，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现。（学习问题） 维特比（Viterbi）算法解决的是给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。例如，通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。 隐马尔可夫对中文分词问题建模与训练例如有3个不同的葫芦，每个葫芦里有好药和坏药若干，现在从3个葫芦中按照以下规则倒出药来。 （1）随机挑选一个葫芦。 （2）从葫芦里倒出一颗药，记录是好药还是坏药后将药放回。 （3）从当前葫芦依照一定的概率转移到下一个葫芦。 （4）重复步骤（2）和（3）。 在整个过程中，我们并不知道每次拿到得是哪一个葫芦。用隐马尔可夫模型来描述以上过程，隐状态就是当前是哪一个葫芦，隐状态的取值空间为{葫芦1，葫芦2，葫芦3}，观测状态的取值空间为{好药，坏药}，初始状态的概率分布就是第（1）步随机挑选葫芦的概率分布，隐状态间的转移概率就是从当前葫芦转移到下一个葫芦的概率，而隐状态到观测状态的输出概率就是每个葫芦里好药和坏药的概率。记录下来药的顺序就是观测状态的序列，而每次拿到的葫芦的顺序就是隐状态的序列。 隐马尔可夫模型通常用来解决序列标注问题，因此也可以将分词问题转化为一个序列标注问题来进行建模。例如可以对中文句子中每个字做以下标注：B表示一个词开头的第一个字，E表示一个词结尾的最后一个字，M表示一个词中间的字，S表示一个单字词，则隐状态的取值空间为{B,E,M,S}。同时对隐状态的转移概率可以给出一些先验知识，B和M后面只能是M或者E，S和E后面只能是B或者S。而每个字就是模型中观测状态，取值空间为语料中的所有中文字。完成建模之后，使用语料进行训练可以分有监督训练和无监督训练。有监督训练即对语料进行标注，相当于根据经验得到了语料的所有隐状态信息，然后就可以用简单的计数法来对模型中的概率分布进行极大似然估计。无监督训练可以用Baum-Welch算法，同时优化隐状态序列和模型对应的概率分布。 因子图与马尔可夫逻辑网马尔可夫逻辑网一个马尔可夫逻辑网就是一个每个准则都有权重的一阶逻辑知识库，可看成是构建马尔可夫逻辑网络的模板。从概率的视角看，马尔可夫逻辑网提供一种简洁的语言来定义大型马尔可夫网，能灵活地、模块化地与大量知识合并；从一阶逻辑的视角看，马尔可夫逻辑网能健全地处理不确定性、容许有瑕疵甚至矛盾的知识库，降低脆弱性。有许多统计关系学习领域的重要任务，如集合分类、链接预测、链接聚合、社会网络建模和对象识别，都自然而然地成为运用马尔科夫逻辑网推理和学习的实例。 马尔可夫网（也叫马尔可夫随机场）是随机变量集$x=x_1,x_2,…,x_n$的联合分布模型，它由一个无向图G和一个势函数$\Psi_k$集合组成，每个随机变量是图上的节点，图的每个团在模型中都有一个势函数，势函数是一个非负实函数，它代表了相应的团的状态。马尔可夫网的联合分布如下：$$P(X= x) =\frac{1}{Z}\prod_k\Psi_k(x_)$$其中$x_$是团中随机变量的状态，Z也叫配分函数（态和），定义为$\sum_{x\in X}\prod_k\Psi_k(x_)$。将马尔可夫网络中每个团的势状态的所有特征值加权后求和再取幂，就可方便地表示成对数线性模式$$P(X=x) = \frac{1}{Z}\exp(\sum_jw_jf_j(x))$$特征函数可以是表示状态的任何实函数。公式是势最直接的表示，其中每个团每个可能的状态都有一个对应的特征值 $f_j(x)$，它的权重是$w_j$，这种表示方法与团数量的幂相关。可是，我们可以自由地运用一些方法比如状态的逻辑函数等减少特征值数量，特别在团数量很大时能相比势函数方式提供一种更简洁的表示形式。马尔可夫逻辑网络就是利用了这一方式。 因子图]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
        <tag>无向图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主题模型专题]]></title>
    <url>%2Fpassages%2F2019-07-25-%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[主题模型产生背景基于词袋模型或N-gram模型的文本表示模型有一个明显的缺陷，就是无法识别出两个不同的词或词组具有相同的主题。因此，需要一种技术能够将具有相同主题的词或词组映射到同一维度上去，于是产生了主题模型。 假设有K个主题，我们就把任意文章表示成一个K维的主题向量，其中向量的每一维代表一个主题，权重代表这篇文章属于这个特定主题的概率。主题模型所解决的事情，就是从文本库中发现有代表性的主题(得到每个主题上面词的分布)，并且计算出每篇文章对应着哪些主题。 常见的主题模型pLSA (Probabilistic Latent Semantic Analysis)pLSA是用一个生成模型来建模文章的生成过程，增加了主题模型，形成简单的贝叶斯网络，可以使用EM算法学习模型参数。 pLSA图模型如下图所示： 假设有K个主题，M篇文章；对语料库中的任意文章d，假设该文章有N个词，则对于其中的每一个词，我们首先选择一个主题z，然后在当前主题的基础上生成一个词w。 （1）D代表文档，Z代表主题（隐含类别），W代表单词；$P(d_i)$表示文档$d_i$的出现概率，$P(z_k|d_i)$表示文档$d_i$中主题$z_k$的出现概率，$P(w_j|z_k)$表示给定主题$z_k$出现单词$w_j$的概率。 （2）每个主题在所有词项上服从多项分布，每个文档在所有主题上服从多项分布。 （3）给定文章d，生成词w的概率可以表示为：$$p(w|d) = \sum_zp(w|z,d)p(z|d)$$这里我们做一个简化，假设给定主题z的条件下，生成词w的概率是与特定的文章无关的，则公式可以简化为：$$p(w|d) =\sum_zp(w|z)p(z|d)$$（4）整个语料库中的文本生成概率可以用似然函数表示为：$$L=\prod_m^M\prod_m^Mp(d_w,w_n)^{c(d_m,w_n)}$$其中$p(d_m,w_n)$是在第m篇文章$d_m$中，出现单词$w_n$的概率，即联合概率分布；$c(d_m,w_m)$是在第m篇文章$d_m$中，单词$w_n$出现的次数。利用对数似然简化计算得：$$L = \sum_m^M\sum_m^Mc(d_m,w_m)\log(d_m,w_n)\= \sum_m^M\sum_n^Mc(d_m,w_n)\log\sum_k^Kp(d_m)p(z_k|d_m)p(w_n|z_k)$$在上面的公式中，定义在文章上的主题分布$p(z_k|d_m)$和定义在主题上的词分布$p(w_n|z_k)$是待估计的参数。由于参数中包含的$z_k$是隐含变量（即无法直接观测到的变量），因此无法利用最大似然估计直接求解，可以利用最大期望算法（EM）来解决。 （5）EM算法的求解过程 需要求解隐变量$z_k$的后验概率，目标函数建立将约束问题求解使用Lagrange乘子法处理。 关于EM算法迭代如下： M-step：$$p(w_j|z_k) = \frac{\sum_{i}n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{m=1}^M\sum_in(d_i,w_j)p(z_k|d_i,w_j)}$$ $$p(z_k|d_i) = \frac{\sum_{j}n(d_i,w_j)p(z_k|d_i,w_j)}{\sum_{k=1}^K\sum_jn(d_i,w_j)p(z_k|d_i,w_j)}$$ E-step：$$p(z_k|d_i,w_j) = \frac{p(w_j|z_k)p(z_k|d_i)}{\sum_{i=1}^Kp(w_j|z_i)p(z_i|d_i)}$$（6）pLSA总结 pLSA应用于信息检索、过滤、自然语言处理等领域，pLSA考虑到词分布和主题分布，使用EM算法来学习参数。 虽然推导略显复杂，但最终公式简洁清晰，很符合直观理解，需用心琢磨；此外，推导过程使用了EM算法，也是学习EM算法的重要素材。 LDALDA可以看作是pLSA的贝叶斯版本，其文本生成过程与pLSA基本相同，不同的是为主题分布分别加了两个狄利克雷(Dirichlet)先验。 为什么要加入狄利克雷先验呢？pLSA采用的是频率派思想，将每篇文章对应的主题分布$p(z_k|d_m)$和每个主题对应的词分布$p(w_n|z_k)$看作确定的未知常数，并可以求解出来；而LDA采用的是贝叶斯学派的思想，认为待估计的参数（主题分布和词分布）不再是一个固定的常数，而是服从一定分布的随机变量。这个分布符合一定的先验概率分布（即狄利克雷分布），并且在观察到样本信息之后，可以对先验分布进行修正，从而得到后验分布。LDA之所以选择狄利克雷分布作为先验分布，是因为它为多项式分布的共轭先验概率分布，后验概率依然服从狄利克雷分布，这样做可以为计算带来便利。下图为LDA的图模型，其中$\alpha,\beta$分别为两个狄利克雷分布的超参数，为人工设定。图中m为文章数，K为主题数，n表示文章的第n个词。 语料库的生成过程为：对文本库中的每一篇文档$d_i$，采用以下操作： （1）从超参数为$\alpha$的狄利克雷分布中抽样生成文档$d_i$的主题分布$\theta_i$。 （2）对文档$d_i$中的每一个词进行以下3个操作。 从代表主题的多项式分布$\theta_i$中抽样生成它所对应的主题$z_{i,j}$。 从超参数为$\beta$的狄利克雷分布中抽样生成主题$z_{i,j}$对应的词分布$\psi_{z_{i,j}}$。 从代表词的多项式分布$\psi_{z_{i,j}}$中抽样生成词$w_{i,j}$。 求解主题分布$\theta_i$以及词分布$\psi_{z_{i,j}}$的期望，可以用吉布斯采样(Gibbs Sampling)的方式实现。 首先随机给定每个单词的主题，然后在其他变量固定的情况下，根据转移概率抽样生成每个单词的新主题。对于每个单词来说，转移概率可以理解为：给定文章中的所有单词以及除自身以外其他所有单词的主题，在此条件下该单词对应为各个新主题的概率。最后，经过反复迭代，我们可以根据收敛后的采样结果计算主题分布和词分布的期望。 如何确定LDA模型中的主题个数在LDA中，主题的个数K是一个预先指定的超参数。对于模型超参数的选择，实践中的做法一般是将全部数据集分为训练集、验证集和测试集三个部分，然后利用验证集对超参数进行选择。 为了衡量LDA模型在验证集和测试集上的效果，需要寻找一个合适的评估指标。一个常用的评估指标是困惑度（perplexity）。在文档集合D上，模型的困惑度被定义为：$$perplexity(D) = exp{-\frac{\sum_{d=1}^Mlogp(w_d)}{\sum_{d=1}^MN_d}}$$其中M为文档的总数，$w_d$为文档d中单词所组成的词袋向量，$p(w_d)$为模型所预测的文档d的生成概率，$N_d$为文档d中单词的总数。 一开始，随着主题数的增多，模型在训练集和验证集的困惑度呈下降趋势，但当主题数目足够大的时候，会出现过拟合，导致困惑度指标在训练集上继续下降但在验证集上反而增长。这时可以取验证集的困惑度极小值点可能出现在主题数目非常大的时候，然而实际应用并不能承受如此大的主题数目，这时需要在实际应用中合理的主题数目范围内进行选择，比如选择合理范围内困惑度的下降明显变慢（拐点）的时候。 另一种方法是在LDA基础上融入分层狄利克雷过程（Hierarchical Dirichlet Process,HDP），构成一种非参数主题模型HDP-LDA。非参数主题模型的好处是不需要预先指定主题的个数，模型可以随着文档数目的变化而自动对主题个数进行调整；它的缺点是在LDA基础上融入HDP之后使得整个概率图模型更加复杂，训练速度也更加缓慢，因此在实际应用中还是经常采用第一种方法确定合适的主题数目。 参考资料 主题模型（概率潜语义分析PLSA、隐含狄利克雷分布LDA）]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
        <tag>pLSA</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯专题]]></title>
    <url>%2Fpassages%2F2019-07-24-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯算法“朴素”的含义朴素贝叶斯模型（Naive Bayesian Model）朴素的含义：建立在两个前提假设上： 特征之间相互独立 每个特征同等重要 然而这种属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果原因在于： （1）对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类； （2）如果属性间依赖对所有类别影响相同，或依赖关系的影响能够相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响 工作原理 假设现在有样本$x = (a_1,a_2,a_3,…a_n)$这个待分类项（并认为x里面是特征独立的） 再假设现在有分类目标$Y = {y_1,y_2,y_3,..y_n}$ 那么$\max(P(y_1|x),P(y_2|x),P(y_3|x)…,P(y_n|x))$就是最终的分类类别 $P(y_i|x) = \frac{P(x|y_i)*P(y_i)}{P(x)}$ 因为x对于每个分类目标来说都一样，所以就是求$\max(P(x|y_i)*P(y_i))$ $P(x|y_i)*p(y_i)= p(y_i)*\prod{P(a_i|y_i)}$ 而具体的$P(a_i|y_i)$和$P(y_i)$都是能从训练样本中统计出来 $P(a_i|y_i)$表示该类别下该特征出现的频率，$P(y_i)$表示全部类别中这个类别出现的概率。 工作流程 准备阶段 确定特征属性，并对每个特征属性适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 训练阶段 计算每个类别在训练样本中的出现频率以及每个特征属性划分对每个类别的条件概率估计 应用阶段 使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别 朴素贝叶斯模型的种类朴素贝叶斯三个常用模型：高斯、多项式、伯努利 高斯模型主要处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度 多项式模型：$$P(x_i|y_k) = \frac{N_{y_kx_i}+\alpha}{N_{y_k}+\alpha n}$$其中$\alpha$是拉普拉斯平滑，加和的是属性出现的总次数，也防止了零概率问题。在文本分类问题中，反映一个词出现的词频，类似投骰子问题n次出现m次这个词的场景。 伯努利模型：伯努利模型特征的取值为布尔型，即出现为True，没有出现为False，在文本分类中，就是一个单词有没有在一个文档中出现。 朴素贝叶斯的应用场景 文本分类/垃圾文本过滤/情感判别 多分类实时预测 推荐系统 朴素贝叶斯和协同过滤是一对好搭档，协同过滤是强相关性，但泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。 贝叶斯决策理论将分类看做决策，进行贝叶斯决策时考虑各类的先验概率和类条件概率，即后验概率。考虑先验概率意味着对样本总体的认识，考虑类条件概率是对每一类中某个特征出现频率的认识。由此不难发现，贝叶斯决策的理论依据就是贝叶斯公式。 最小错误率贝叶斯决策贝叶斯决策的基本理论依据就是贝叶斯公式，判决遵从最大后验概率。这种仅根据后验概率作决策的方式称为最小错误率贝叶斯决策，可以从理论上证明这种决策的平均错误率是最低的。 最小风险贝叶斯决策另一种方式是考虑决策风险，加入了损失函数，称为最小风险贝叶斯决策。 朴素贝叶斯三种常用的分类模型朴素贝叶斯的三个常用模型：高斯、多项式、伯努利。 高斯模型主要处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度； 多项式模型：$$P(x_i|y_k) = \frac{N_{y_kx_i}+\alpha}{N_{y_k}+\alpha n}$$其中$\alpha$是拉普拉斯平滑，加和的是属性出现的总次数，也防止了零概率问题。在文本分类问题中，反映一个词出现的词频，类似投骰子问题n次出现m次这个点数的场景。 伯努利模型： 伯努利模型特征的取值为布尔型，即出现为true，没有出现为false，在文本分类中，就是一个单词有没有在一个文档中出现。 朴素贝叶斯细节问题零概率问题描述：在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果为0。 解决方案：通常解决这个问题的方法是要进行平滑处理，常用拉普拉斯修正。 拉普拉斯修正的含义是，在训练集中总共的分类数，用 N 表示；di 属性可能的取值数用 $N_i$ 表示，因此， 原来的先验概率$P(c)$的计算公式由：$P(c) = \frac{D_c}{D}$ 被拉普拉斯修正为：$P(c) = \frac{D_c+1}{D+N}$ 类的条件概率$P(x|c)$的计算公式由：$P(x_i|c) = \frac{D_{c,st}}{D_c}$ 被拉普拉斯修正为：$P(x_i|c) = \frac{D_{c,st}+1}{D_c+N_i}$ 使用拉普拉斯平滑，拉普拉斯因子的大小如何确定？朴素贝叶斯中的拉普拉斯因子$\alpha$无法通过公式求出最优大小，需要根据程序员的经验去设置，使用交叉验证的方式求取最优大小。 下溢问题问题描述：在计算过程中，需要对特定分类中各个特征出现的概率进行连乘，小数相乘，越乘越小，造成下溢出，计算结果变成0。 解决方案：通过log运算增大概率的绝对值。log运算不会影响函数的趋势和极值只是扩大值得范围。将小数的乘法操作转化为取对数后的加法操作，规避了变为零的风险同时并不影响分类结果。 异常值敏感问题朴素贝叶斯是一种对异常值不敏感的分类器，保留数据中的异常值，常常可以保持贝叶斯算法的整体精度，如果对原始数据进行降噪训练，分类器可能会因为失去部分异常值的信息而导致泛化能力下降。 异常值不敏感原因：可能是因为朴素贝叶斯分类器是一个概率模型，如果输入是一个正常的样本，则异常值并不会影响到正常样本的后验概率。因为对于正常样本而言$p(x|y_i)*p(y_i)=p(y_i)\prod_j{p(x_j|y_i)}$，其中$x_j$是正常的，并不会使用到异常值。如果是一个异常的$p(x_j|y_i)$，反而已有的异常值可以帮助到该异常样本更好的分类。 异常值有影响的情况：如果是对于连续型属性的异常值则会产生对分类器产生一定的影响，因贝叶斯对连续值的处理往往是通过估计其概率分布的参数，若有异常值存在则其概率分布将会产生偏移。若是分类变量则之间统计出现次数是不会产生偏移的。 缺失值敏感问题不敏感 原因：朴素贝叶斯算法能够处理缺失的数据，在算法的建模时和预测时数据的属性都是单独处理的。因此如果一个数据实例缺失了一个属性的数值，在建模时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。 数据的属性是连续型变量的情况当朴素贝叶斯算法数据的属性为连续型变量时，有两种方法可以计算属性的类条件概率。 第一种方法是把一个连续的属性离散化，然后用相应的离散区间替换连续属性值，之后用频率去表示类条件概率。但这种方法不好控制离散区间划分的粒度。如果粒度太细，就会因为每个区间内训练记录太少而不能对做出可靠估计，如果粒度太粗，那么有些区间就会有来自不同类的记录，因此失去了正确的决策边界。 第二种方法是假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，例如可以使用高斯分布来表示连续属性的类条件概率分布。$$P(X_i = x_i|Y=y_i) =\frac{1}{\sqrt{2\pi\sigma_{i,j}}}e^{-\frac{(x_i-\mu_{i,j})^2}{2\sigma_{i,j}^2}}$$其中$\mu_{i,j}$为类$y_j$的所有训练记录关于$X_i$的样本均值估计，$\sigma_{i,j}^2$为类$y_i$的所有训练记录关于$X_i$的样本方差估计。通过高斯分布估计出类条件概率。 高度相关的特征对朴素贝叶斯的影响假设有两个特征高度相关，相当于该特征在模型中发挥了两次作用（计算两次条件概率），使得朴素贝叶斯获得的结果向该特征所希望的方向进行了偏移，影响了最终结果的准确性，所以朴素贝叶斯算法应先处理特征，把相关特征去掉。 朴素贝叶斯的增量计算传统的贝叶斯方法在有新的训练样本加入时，需要重新学习已经学习过的样本，耗费大量时间。增量计算就是在原有分类器的基础之上，自主选择学习新的文本来修正分类器。因为朴素贝叶斯在训练过程中实际只需要计算出各个类别的概率（先验）和各个特征的类条件概率，这些概率值可以快速的根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算，该特性可以使用在超出内存的大量数据计算和随时间等（流数据）获取数据的计算中。 朴素贝叶斯总结朴素贝叶斯是高偏差低方差在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为Error = Bias+Variance。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望（平均值）之间的误差，即模型的稳定性，数据是否集中。 对于朴素贝叶斯，它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型，对于复杂模型，充分拟合了部分数据，使得他们的偏差较小，而由于对部分数据的过度拟合，对于部分数据预测效果不好，整体来看可能引起方差较大，简单模型与之相反，大部分场合偏差部分大于方差部分，也就是高偏差低方差。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 朴素贝叶斯的优缺点优点： 对数据的训练快，分类也快 对缺失数据不太敏感，对异常值也不太敏感，算法也比较简单 对小规模的数据表现很好，能够处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练 缺点： 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。 对输入数据的表达形式很敏感。（离散的类别之间统计频率即可，连续值就要估计概率分布。） 参考资料 机器学习面试题之NB——朴素贝叶斯]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型概述]]></title>
    <url>%2Fpassages%2F2019-07-23-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[概率图概率图中的节点分为隐含节点和观测节点，边分为有向边和无向边。从概率论的角度，节点对应于随机变量，边对应于随机变量的依赖或相关关系，其中有向边表示单向的依赖，无向边表示相互依赖关系。 概率图模型分为贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）两大类。贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。更详细地说，概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在机器学习的诸多场景中都有着广泛的应用。 马尔可夫网络的联合概率分布在马尔可夫网络中，联合概率分布的定义为$$P(x) = \frac{1}{Z}\prod_{Q \in C}\varphi_{Q}(X_Q)$$其中$C$为图中最大团所构成的集合，$Z = \sum_{x} \prod_{Q \in C}\varphi_{Q}(X_Q)$为归一化因子，用来保证$P(x)$是被正确定义的概率，$\varphi_{Q}$是与团$Q$对应的势函数。势函数是非负的，并且应该在概率较大的变量上取得较大的值，例如指数函数$$\varphi_{Q}(X_Q)=e^{-H_{Q}(x_Q)}$$其中，$$H_Q(x_Q)= \sum_{\mu,v \in Q,\mu \neq v}\alpha_{\mu,v}x_{\mu}x_v+\sum_{v \in Q}\beta_vx_v$$对于图中所有节点$x={x_1,x_2,…,x_n}$所构成的一个子集，如果在这个子集中，任意两点之间都存在边相连，则这个子集中的所有节点构成了一个团。如果在这个子集中加入任意其他节点，都不能构成一个团，则称这样的子集构成了一个最大团。 对于由A，B，C，D四个点构成的四边形无向图，其联合概率分布为：$$P(A,B,C,D)=\frac{1}{Z}e^{-H(A,B,C,D)}$$ 概率图表示解释朴素贝叶斯模型原理朴素贝叶斯模型通过预测指定样本属于特定类别的概率$P(y_i|x)$来预测该样本的所属类别，即$$y = \max_{y_i}P(y_i|x)$$$P(y_i|x)$可以写成$$P(y_i|x) = \frac{P(x|y_i)P(y_i)}{P(x)}$$其中$x= (x_1,x_2,x_3,…,x_n)$为样本对应的特征向量，$P(x)$为样本的先验概率。对于特定的样本x和任意类别$y_i$，$P(x)$的取值均相同，在计算中可以被忽略。假设特征相互独立，$P(y_i)$可以通过训练样本统计得到，后验概率$P(x_j|y_i)$的取值决定了分类的结果。并且任意特征$x_j$都由$y_i$的取值所影响。 解释最大熵模型的原理信息是指人们对事物理解的不确定性的降低或消除，而熵是不确定性的度量，熵越大，不确定性也就越大。 最大熵原理是概率模型学习的一个准则，指导思想是在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。 在对训练数据集一无所知的情况下，最大熵模型认为$P(y|x)$是符合均匀分布的。 给定离散随机变量$x$和$y$上的条件概率分布$P(x|y)$，定义在条件概率分布上的条件熵为：$$H(p) = -\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)$$其中$\tilde{P}(x)$为样本在训练数据集上的经验分布，即$x$的各个取值在样本中出现的频率统计。 当我们有了训练数据集之后，我们希望从中找到一些规律，从而消除一些不确定性，这时就需要用到特征函数$f(x,y)$。特征函数$f$描述了输入$x$和输出$y$之间的一个规律。为了使学习到的模型能够正确捕捉训练数据集中的特征，我们加入一个约束，使得特征函数$f(x,y)$关于经验分布$\tilde{P}(x,y)$的期望值与关于模型$P(y|x)$和经验分布$\tilde{P}(x)$的期望值相等。$$E_{\tilde{P}}(f) = \sum_{x,y}\tilde{P}(x,y)f(x,y)$$ $$E_p(f) = \sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$ 综上，给定训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，以及M个特征函数${f_i(x,y),i=1,2,…,M}$，最大熵模型的学习等价于约束最优化问题：$$\max_{p}H(P) = -\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\ s.t., E_{\tilde{P}}(f) = E_p(f), \forall i =1,2,…,M,\ \sum_{y}P(y|x) = 1$$求解之后可以得到最大熵模型的表达形式为$$P_w(y|x) = \frac{1}{Z}exp(\sum_{i=1}^{M}w_if_i(x,y))$$最终，最大熵模型归结为学习最佳的参数$w$，使得$P_w(y|x)$最大化。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯混合模型]]></title>
    <url>%2Fpassages%2F2019-07-22-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[高斯混合模型高斯混合模型（Gaussian Mixed Model，GMM）是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都符合高斯分布（又叫正态分布）。 高斯混合模型的核心思想高斯混合模型的核心思想是，假设数据可以看作多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值$\mu_i$和方差$\sum_i$是待估计的参数。此外，每个分模型都还有一个参数$\pi_i$，可以理解为权重或生成数据的概率。高斯混合模型的公式为：$$p(x) = \sum_{i=1}^K\pi_iN(x|\mu_i,\sum_i)$$ 高斯混合模型与K均值算法的异同高斯混合模型与K均值算法的相同点是，它们都是可用于聚类算法；都需要指定K值；都是使用EM算法来求解；都往往只能收敛于局部最优。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>面试笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K均值聚类]]></title>
    <url>%2Fpassages%2F2019-07-22-k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[K均值的基本思想k均值是最基础和最常用的聚类算法。它的基本思想是通过迭代方式寻找K个簇（cluster）的一种划分方案，使得聚类结果对应的代价函数最小。代价函数可以定义为各个样本距离所属簇中心点的误差平方和：$$J(c,\mu) = \sum_{i=1}^{M}||x_i-\mu_{c_i}||^2$$其中$x_i$代表第i个样本，$c_i$是$x_i$所属于的簇，$\mu_{c_i}$代表簇对应的中心点，$M$是样本总数。 K均值算法的优缺点是什么？如何对其进行调优？k均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类等。但是瑕不掩瑜，k均值聚类的优点也很明显：主要体现在：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是O(NKt)近乎线性，其中N是数据对象的数目，K是聚类的簇数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。 K均值算法的调优一般可以从以下几个角度出发。 （1）数据归一化和离群点处理 K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。 （2）合理选择K值 K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。K值的选择一般基于经验和多次实验结果。例如采用手肘法，K值越大，距离和越小。手肘法认为拐点就是K的最佳值。 了解：Gap Statistic算法 （3）采用核函数 传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。 针对K均值算法的缺点，有哪些改进的模型？K均值算法的主要缺点： （1） 需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。 （2） K均值只能收敛到局部最优，效果受到初始值影响很大。 （3） 易受到噪点的影响。 （4） 样本点只能被划分到单一的类中。 K-means++算法： 在原始K均值算法的随机选择聚集中心的基础上改进，后面过程中都是一样的。K-means++按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心（0&lt;n&lt;K），则在选择第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。 ISODATA算法： 当K值的大小不确定时，可以使用ISODATA算法。当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA针对这个问题进行了改进。ISODATA的全称为迭代自组织数据分析法。 ISODATA算法在K均值算法的基础之上增加了两个操作：一是分裂操作，对应着增加聚类中心数；二是合并操作，对应减少聚类中心数。其缺点时需要指定的参数比较多。ISODATA算法的各个输入参数： （1）预期的聚类中心数目$K_o$。最终输出的聚类中心数目常见范围是从$K_o$的一半，到两倍$K_o$。 （2）每个类所要求的最少样本数目$N_{min}$。如果分裂后会导致某个子类别包含样本数目小于该阈值，就不会对该类别进行分裂操作。 （3）最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足（2），进行分裂操作。 （4）两个聚类中心之间所允许最小距离$D_{min}。$如果两个类靠的非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，则对两个类进行合并操作。 如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。 最大期望算法（EM算法）K均值聚类的迭代算法实际上是一种最大期望算法。简称EM算法。EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。由于隐变量是未知的，无法直接通过最大似然估计求解参数，这时需要利用EM算法来求解。 EM算法框架总结如下，由以下两个步骤交替进行直到收敛： （1）E步骤：计算隐变量的期望$$Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)},\theta)$$（2） M步骤：最大化，目的是通过最大化这个下界可以使得待优化函数向更好的方向改进。$$\theta = argmax\sum_{i=1}^{m}\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)}| \theta)}{Q_i(z^{(i)})}$$K均值算法等价于用EM算法求解含隐变量的最大似然问题。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec知识简述]]></title>
    <url>%2Fpassages%2F2019-07-08-Word2Vec%2F</url>
    <content type="text"><![CDATA[背景谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别是CBOW（Continues Bag of Words）和Skip-gram。 负采样（Negative Sample）和层次softmax（Hierarchical Softmax）则是两种加速训练的方法。 工作流程CBOW的目标是根据上下文出现的词语来预测当前词的生成概率；而Skip-gram是根据当前词来预测上下文中各词的生成概率。 其中$w(t)$是当前所关注的词，$w(t-2)、w(t-1)、w(t+1)、w(t+2)$是上下文中出现的词。这里前后滑动窗口大小均设为2。 CBOW和Skip-gram都可以表示成由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络。 输入层中的每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。 在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由N维输入向量以及连接输入和隐含单元之间的N*K维权重矩阵计算得到。在CBOW中，还需要将各个输入词所计算出的隐含单元求和。 同理，输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的K*N维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用softmax激活函数，可以计算出每个单词的生成概率。Softmax函数定义为：$$p(y=w_n|x) = \frac{e^{x_{n}}}{\sum_{k=1}^{N}e^{x_{k}}}$$其中$x$代表N维的原始输出向量，$x_n$为在原始输出向量中，与单词$w_n$所对应维度的取值。 接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为N*K的权重矩阵，从隐含层到输入层需要一个维度K*N的权重矩阵，学习权重可以用反向传播算法实现。但由于Softmax中存在归一化项的缘故，推导出的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，因此需要改进。最终训练得到维度为N*K和K*N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。 负采样最后的softmax分出来的类是整个词袋的大小，那么是不是可以把词袋大小减小，因为有些出现概率低的词我们根本可以不考虑。这就是负采样的核心思想。 假设我们有一个训练样本，中心词是$w$，它周围上下文共有$2c$个词，记为$context(w)$。由于这个中心词$w$的确和$content(w)$相关存在，因此它是一个真实的正例。通过Negative Sampling 采样，得到n个和$w$不同的中心词$w_i,i =1,2,…,n$，这样$context(w)$和$w_i$就组成了n个并不存在的负例。利用这一个正例和n个负例，进行二元逻辑回归，得到负采样对应的每个词$w_i$对应的模型参数$\theta_i$和每个词的词向量。整个过程相比Hierarchical Softmax简单。 但需要明确两个问题：（1）如何通过一个正例和n个负例进行二元逻辑回归？（2）如何进行负采样？ 梯度计算这里我们将正例定义为$w_0$。 在逻辑回归中，正例期望满足：$$P(context(w_0),w_i)=\sigma(x_{w_0}^T\theta^{w_i}),y_i=1,i=0$$负期望满足：$$P(context(w_0),w_i)=1-\sigma(x_{w_0}^T\theta^{w_i}),y_i=0,i=1,2,..n$$其对应的对数似然函数为：$$L= \sum_{i=0}^ny_ilog(\sigma(x_{w_0}^T\theta^{w_i}))+(1-y_i)log(1-\sigma(x_{w_0}^T\theta^{w_i}))$$和层次Softmax类似，采用随机梯度上升法，仅仅每次只用一个样本更新梯度，来进行迭代更新得到我们需要的$x_{w_i},\theta^{w_i}$。求导得梯度表达式，用梯度上升法进行一步步的求解。 负采样方法word2vec采用的方法不复杂，如果词汇表的大小为V，那么我们就将一段长度为1的线段分成V份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短，每个词$w$的线段长度由下式决定：$$len(w) =\frac{count(w)}{\sum_{\mu\in vocab}count(\mu)}$$在word2vec中，分子和分母都取了3/4次幂如下：$$len(w) =\frac{count(w)^{3/4}}{\sum_{\mu\in vocab}count(\mu)^{3/4}}$$ 在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M&gt;&gt;V$，这样可以保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从$M$个位置中采样出$n$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。 在word2vec中，$M$取值默认为$10^8$ 层次softmaxHierarchical Softmax，层次softmax是一种加速训练的技巧，要解决的问题是原来softmax的参数太多的问题，所以不管是层次softmax也好，还是负采样也好，都是对最后的softmax做一个处理。 层次softmax的核心内容是哈夫曼树（Huffman Tree）。树的核心概念是 出现概率越高的符号使用较短的编码（层次越浅），出现概率低的符号则使用较长的编码（层次越深）。 缺点使用霍夫曼树来替代传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。负采样比之而言考虑了该问题，不再使用霍夫曼树。 参考资料 《百面机器学习》 word2vector的原理，结构，训练过程 word2vec原理]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>NLP</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程知识点整理概述(二)]]></title>
    <url>%2Fpassages%2F2019-07-07-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%E6%A6%82%E8%BF%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[文本表示文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。 词袋模型（Bag of Words）最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重。TF-IDF内容详见我的博客：TF-IDF算法介绍 将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的natural language processing（自然语言处理）一词，如果将natural，language，processing这三个词拆分开来，所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的n个词$(n\le N)$组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（word Stemming）处理，即将不同词性的单词统一成为同一词干形式。 主题模型（Topic Model）主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。具体处理方法在后续的博文中楼主会详细介绍…… 词嵌入模型 （Word Embedding）词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K = 50~300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>NLP</tag>
        <tag>文本表示</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程知识点整理概述（一）]]></title>
    <url>%2Fpassages%2F2019-07-06-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[特征归一化为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。 数值类型特征常用归一化方法最常用的方法主要有以下两种： （1）线性函数归一化 (Min-Max Scaling) 它对原始数据进行线性变换，使结果映射到$[0,1]$的范围，实现对原始数据的等比缩放。归一化公式如下：$$X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}$$其中$X$为原始数据，$X_{max}、X_{min}$分别为数据最大值和最小值。 （2）零均值归一化（Z-Score Normalization） 它将原始数据映射到均值为0、标准差为1的分布上。具体来说，假设原始特征的均值为$\mu$、标准差为$\alpha$的分布上，那么归一化公式定义为：$$z = \frac{x-\mu}{\sigma}$$数据归一化不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但对于决策树模型则并不适用。 类别型特征处理办法类别型特征原始输入通常是字符串形式，除了决策树等少数模型能够直接处理字符串形式，对于逻辑回归、支持向量机等模型来说，类别型的特征必须经过处理转换成数值型特征才能正确工作。 常用的类别型特征处理方法如下： 序号编码序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三档，并且“高&gt;中&gt;低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID。 独热编码独热编码通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用独热编码需要注意以下问题。 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位置均取值为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。 二进制编码二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。 其他编码方式还有很多，比如Helmert Constrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast等。 组合特征组合特征定义为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。 以逻辑回归为例，假设数据的向量特征为$X = (x_1,x_2,…,x_k)$，则有$$Y = sigmoid(\sum_i\sum_yw_{ij}&lt;x_i,x_j&gt;)$$其中$&lt;x_i,x_j&gt;$表示$x_i$和$x_j$的组合特征，$w_{ij}$的维度等于$|x_i|\cdot|x_j|$，$|x_i|$和$|x_j|$分别代表第$i$个特征和第$j$个特征不同取值的个数。 对于推荐算法，若用户数量为$m$，物品数量为$n$，那么需要学习参数的规模为$m*n$。在互联网环境下，用户数量和物品数量都可以达到千万量级，几乎无法学习$m*n$规模的参数。在这种情况下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示$(k&lt;&lt;m,k&lt;&lt;n)$$$Y = sigmoid(\sum_i\sum_yw_{ij}&lt;x_i,x_j&gt;)$$这里$w_{ij}=x_{i}^{‘} \cdot x_{j}^{‘}$，$x_{i}^{‘}$和$x_{j}^{‘}$分别表示$x_i$和$x_j$ 对应的低维向量。那么在推荐问题中，需要学习的参数的规模变为$m*k+n*k$，这其实等价于矩阵分解。这就是利用降维方法来减少两个高维特征组合后需要学习的参数。 怎样有效地找到组合特征实际问题中并不是所有的特征组合都是有意义的。这里介绍一种基于决策树的特征组合寻找方法。于是，每一条从根节点到叶节点的路径都可以看成一种特征组合的方式。假设我们有4种特征组合方式，如果样本满足1，2特征组合方式，则其特征表示可以表示为$(1,1,0,0)$。 对于利用原始数据有效的构造决策树，我们可以采用基于boosting的GBDT梯度提升决策树，该方法的思想是每次都在之前构造的决策树的残差上构建下一棵决策树。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>特征工程</tag>
        <tag>归一化</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF算法介绍]]></title>
    <url>%2Fpassages%2F2019-07-05-TF-IDF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[TF-IDF定义TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种加权技术。采用一种统计方法，根据字词在文本中出现的次数和在整个语料中出现的文档频率来计算一个字词在整个语料中的重要程度。 TF-IDF的主要思想TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF是词频(Term Frequency)词频（TF）表示词条（关键字）在文本中出现的频率。 这个数字通常会被归一化（一般是词频除以文章总词数），以防止它偏向长的文件。其公式如下：$$tf_{ij}=\frac{n_{i,j}}{\sum_kn_{k,j}}$$ $$TF_w=\frac{在某一类中词条w出现的次数}{该类中所有的词条数目}$$ 其中$n_{i,j}$是该词在文件$d_j$中出现的次数，分母则是文件$d_j$中所有词汇出现的次数总和。 IDF是逆向文件频率(Inverse Document Frequency)逆向文件频率（IDF）：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。 如果包含词条$t$的文档越少，IDF越大，则说明词条具有很好的类别区分能力。其公式如下：$$idf_{i}=\log{\frac{|D|}{|{j:t_i\in d_j}|}}$$其中，$|D|$ 是语料库中的文件总数。 $|{j:t_i\in d_j}|$表示包含词语 $t_i$ 的文件数目（即 $n_{i,j}≠0$ 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$1+|{j:t_i\in d_j}|$ ，即：$$IDF = \log(\frac{语料库的文档总数}{包括词条w的文档数+1})$$ TF-IDF实际上是：TF * IDF某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。$$TF-IDF = TF*IDF$$Note：TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。 TF-IDF的应用 搜索引擎 关键词提取 文本相似性 文本摘要 python sklearn工具包下的TF-IDF调用 1class sklearn.feature_extraction.text.TfidfTransformer(norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False) norm：词向量归一化 参考资料 TF-IDF算法介绍及实现 文本特征提取之TFIDF与Word2Vec]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>文本表示</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2Fpassages%2F2019-07-04-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[定义动态规划与分治方法相似，都是通过组合子问题的解来求解原问题。分治方法将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解组合起来，求出原问题的解。与之相反，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题（子问题的求解是递归进行的，将其划分为更小的子子问题）。动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，避免了这种不必要的计算工作。 动态规划方法通常用来求解最优化问题。这类问题可以有很多可行解，每个解都有一个值，我们希望寻找具有最优值的解。我们称这样的解为问题的一个最优解，因为可能有多个解都达到最优值。 设计方法通常按如下4个步骤来设计一个动态规划算法： 刻画一个最优解的结构特征。 递归地定义最优解的值。 计算最优解的值，采用自底向上的方法。 利用计算出的信息构造一个最优解。 经典题目——最长公共子序列问题(LCS)与最长公共子串问题最长公共子序列问题（LCS问题）给定两个字符串A和B，长度分别为m和n，要求找出它们最长的公共子序列，并返回其长度。例如： A = "HelloWorld" B="loop" 则A与B的最长公共子序列为 “loo”,返回的长度为3。此处只给出动态规划的解法：定义子问题$dp[i][j]$为字符串A的第一个字符到第 i 个字符串和字符串B的第一个字符到第 j 个字符的最长公共子序列。 为了求解$dp[i][j]$，我们要先判断A的第i个元素B的第j个元素是否相同即判断A[i - 1]和 B[j -1]是否相同，如果相同它就是$dp[i-1][j-1]+ 1$，相当于在两个字符串都去掉一个字符时的最长公共子序列再加 1；否则最长公共子序列取$dp[i][j-1]$和$dp[i-1][j]$中大的那个。 问题的初始状态为：$$dp[i][0] = 0,dp[0][j] = 0$$相应的状态转移方程为：$$dp[i][j] =\begin{cases}\max{(dp[i-1][j],dp[i][j-1])}&amp; A[i-1]!=B[j-1]\\dp[i-1][j-1]+1&amp;A[i-1] == B[j-1]\end{cases}$$ 代码实现如下： 123456789101112131415161718def LongestSubstring(str1,str2): res = 0 str1_len = len(str1) str2_len = len(str2) if str1_len == 0 or str2_len == 0: return res dp = [[0 for j in range(str1_len)] for i in range(str1_len)] for i in range(str1_len): for j in range(str2_len): if str1[i] == str2[j]: if i == 0 or j == 0: dp[i][j] = 1 else: dp[i][j] = dp[i-1][j-1]+1 else: dp[i][j] = max(dp[i-1][j],dp[i][j-1]) return dp[i][j] 该算法的时间复杂度为$O(n*m)$，空间复杂度为$O(n*m)$。 Note：需要考虑空串的判定条件 最长公共子串问题给定两个字符串A和B，长度分别为m和n，要求找出它们最长的公共子串，并返回其长度。例如： A = "HelloWorld" B = "loop" 则A与B的最长公共子串为 "lo",返回的长度为2。我们可以看到子序列和子串的区别：**子序列和子串都是字符集合的子集，但是子序列不一定连续，但是子串一定是连续的**。 整个问题的初始状态为：$$dp[i][0] = 0,dp[0][j] = 0$$相应的状态转移方程为：$$dp[i][j] =\begin{cases}0&amp; A[i-1] !=B[j-1]\\dp[i-1][j-1]+1 &amp; A[i-1] == B[j-1]\end{cases}$$代码实现如下： 1234567891011121314151617def lcsubstr(str1,str2): res = 0 str1_len = len(str1) str2_len = len(str2) if str1_len == 0 or str2_len == 0: return res dp = [[0 for j in range(str1_len)] for i in range(str1_len)] for i in range(str1_len): for j in range(str2_len): if str1[i] == str2[j]: if i == 0 or j == 0: dp[i][j] = 1 else: dp[i][j] = dp[i-1][j-1]+1 if dp[i][j]&gt; res: res = dp[i][j] return res 该算法的时间复杂度为$O(n*m)$ ，空间复杂度为$O(n*m)$。 Note：需要考虑空串的判定条件 参考资料 《算法导论第三版》]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>经典算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制整理]]></title>
    <url>%2Fpassages%2F2019-07-03-Attention%E6%9C%BA%E5%88%B6%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Attention的定义与作用按照Stanford大学课件上的描述，attention的通用定义如下： 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。 attention的重点就是这个集合values中的每个value的“权值”的计算方法。 有时候也把这种attention的机制叫做query的输出关注了原文的不同部分。（Query attends to the values） 换句话说，attention机制就是一种根据某些规则或者某些额外信息（query）从向量表达集合（values）中抽取特定的向量进行加权组合（attention）的方法。简单来讲，只要我们从部分向量里面搞了加权求和，那就算用了attention。 Attention-based Model其实就是一个相似性的度量，当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入。 参考资料 浅谈attention机制 自然语言处理中的Attention机制总结]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[作业帮算法卷笔试]]></title>
    <url>%2Fpassages%2F2019-07-02-%E4%BD%9C%E4%B8%9A%E5%B8%AE%E7%AE%97%E6%B3%95%E5%8D%B7%E7%AC%94%E8%AF%95%2F</url>
    <content type="text"><![CDATA[博主问题笔记不包含博主经历所有题目只记录典型问题（估计也差不多全部了，因为博主菜…..） 问题1：交叉熵公式 解答：交叉熵公式如下：$$H(x,y) = -\sum_{i=1}^nx_i\ln{y_i}$$这里公式定义，x、y都是表示概率分布。其中x是正确的概率分布，而y是我们预测出来的概率分布，这个公式算出来的结果，表示y与正确答案x之间的错误程度（即：y错得有多离谱），结果值越小，表示y越准确，与x越接近。 问题2：随机森林与GDBT的异同 解答： 理论知识随机森林随机森林是一个用随机方式建立的，包括多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。假设样本总数为n，每个样本的特征数为a。则随机森林生成过程如下： 从原始样本中采用有放回抽样的方法选取n个样本; 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点； 重复m次，获取m个决策树； 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。 随机森林的随机性主要体现在两个方面： 数据集的随机选取：从原始的数据集中采取有放回的抽样(bagging)，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。 待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选择最优的特征。 随机森林的优点： 实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的； 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合； 能够处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的； 对于不平衡的数据集，可以平衡误差； 相比SVM，不是很怕特征缺失，因为待选特征也是随机选取； 训练完成后可以给出哪些特征比较重要。 随机森林的缺点： 在噪声过大的分类和回归问题还是容易过拟合； 相比于单一决策树，它的随机性让我们难以对模型进行解释。 GBDTGBDT （Gradient Boost Decision Tree）是以决策树为学习器迭代算法，注意GDBT里的决策树都是回归树而不是分类树。Boost是“提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。 GBDT的核心就在于：每一棵树学的是之前所有树结果和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。GBDT优点是适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。缺点是由于弱分类器的串行依赖，导致难以并行训练数据。 随机森林和GBDT的区别 随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回均匀取样，而boosting根据错误率来取样（Boosting 初始化时对每一个训练样例赋相等的权重$\frac{1}{n}$，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样本赋以较大的权重），因此Boosting的分类京都要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而boosting的训练集的选择与前一轮的学习结果有关，是串行的。 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。 组成随机森林的树可以并行生成；而GBDT只能是串行生成。 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。 随机森林对异常值不敏感；GDBT对异常值非常敏感。 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能。 参考资料：机器学习：随机森林和GDBT的区别 问题3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于$P(B) =\frac{P(AB)}{P(A)}$求后验概率问题。 问题4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为$O(d(r+n))$被忽略其线性。 问题5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题6：剑指Offer原题，螺旋遍历 解答：主要找规律找出循环条件：$columns \gt startX * 2$并且$rows \gt startY*2$。 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding:utf-8 -*-class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): # write code here if matrix is None: return res = [] start = 0 column = len(matrix[0]) row = len(matrix) while column &gt; 2*start and row &gt; start*2: self.PrintMatrixIncicle(matrix,column,row,start,res) start += 1 return res def PrintMatrixIncicle(self,matrix,column,row,start,res): endX = column-1-start endY = row-1-start i = start while i &lt;= endX: number = matrix[start][i] self.printNumber(number,res) i += 1 if start &lt; endY: i = start+1 while i &lt;= endY: number= matrix[i][endX] self.printNumber(number,res) i += 1 if start &lt; endX and start &lt; endY: i = endX-1 while i &gt;= start: number = matrix[endY][i] self.printNumber(number,res) i -= 1 if start &lt; endX and start &lt; endY-1: i = endY-1 while i &gt;= start+1: number = matrix[i][start] self.printNumber(number,res) i -= 1 def printNumber(self,number,res): res.append(number)]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Line实习面]]></title>
    <url>%2Fpassages%2F2019-07-01-Line%E5%AE%9E%E4%B9%A0%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1. 笔试问题1：平衡二叉树的性质（笔者菜，画蛇添足了…） 正确解答：平衡二叉树或者是棵空树，或者是具有下列性质的二叉树： 它的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值不超过1。 若将二叉树节点的平衡因子（Balance Factor）定义为该节点的左子树的深度减去它的右子树的深度，则平衡二叉树上所有节点的平衡因子只可能为-1，0，1。 只要二叉树上有一个节点的平衡因子的绝对值大于1，那么这颗平衡二叉树就失去平衡了 问题2：在浏览器中输入 www.xxxx.com 后执行的全部过程。 正确解答： 客户端浏览器通过DNS解析到 www.xxxx.com 的IP地址，然后通过TCP进行封装数据包，输入到网络层。 在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 客户端的网络层不必关心应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，就是通过查找路由表决定通过哪个路径到达服务器。 客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换IP数据包。现在就可以传输了，然后发送IP数据包到达服务器的地址。 问题3： 设计实现HashTable类。(这题博主见过两次了，都没答对，是真的菜…) HashTable有两个问题需要解决，首先是key值哈希化，我们可以借助Python自带的hash函数解决key的哈希编码问题。其次是Hash 冲突的解决机制，在这里只考虑最简单的一种，即将同一个 hash 值下的不同的 key 存放在数组的同一个位置，以链表形式保存。 123456789101112131415161718192021222324252627282930class MyDict(object): def __init__(self,size = 10000): self.hash_list = [list() for _ in range(size)] self.size = size def __setitem__(self,key,value): # 利用python自带的hash函数，对key哈希并对size取模 # hashed_key位置没有值就追加，否则覆盖 hashed_key = hash(key)%self.size for item in self.hash_list[hashed_key]: if item[0] == key: item[1] = value break else: self.hash_list[hashed_key].append([key,value]) def __getitem__(self,key): # return: key所对应的value # 没有key，就抛出keyError for item in self.hash_list[hash(key)%self.size]: if item[0] == key: return item[1] raise keyError(key) def __repr__(self): # hashtable打印 result = [] for sub_list in self.hash_list: if not sub_list: continue for item in sub_list: result.append(str(item[0])+&quot;:&quot;+str(item[1])) return &quot;&#123;&quot;+&quot;,&quot;.join(result)+&quot;&#125;&quot; 参考答案：不用 Python 自带的 Dict 实现自己的 HashTable 2. 面试主要问博主与简历相关的工作与能力，后续博主会陆续更新自己的研究内容。（博主过了，但不能实习可惜…）]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce原理与排序应用]]></title>
    <url>%2Fpassages%2F2019-07-01-MapReduce%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. MapReduce工作机制MapReduce执行总流程 MapReduce Framework如上图所示。 JobTracker: 初始化作业，分配作业，与TaskManager通信，协调整个作业的执行 TaskTracker: 保持与JobTracker的通信，执行map或者reduce任务 HDFS；保存作业的数据，配置信息等，保存作业结果 具体相关流程提交作业客户端编写完程序代码后，打成jar，然后通过相关命令向集群提交自己想要跑的MR任务，具体过程如下： 通过调用JobTracker的getNewJobId()获取当前作业id 检查作业相关路径 计算作业的输入划分，并将划分信息写到Job.split文件中 将运行作业所需要的资源包括作业jar包，配置文件和打算所得的输入划分，复制到作业对应的HDFS上 调用JobTracker的summitJob()提交，告诉JobTracker作业准备执行 初始化作业 从HDFS中读取作业对应的job.split，得到输入数据的划分信息 创建并且初始化Map任务和Reduce任务：为每个map/reduce task生成一个TaskInProgress去监控和调度该task。例如创建两个初始化task，一个初始化Map，一个初始化Reduce 分配任务JobTracker会将任务分配到TaskTracker去执行，但是怎么判断哪些TaskTracker，怎么分配任务呢？所以，我们要实现JobTracker和TaskTracker中的通信，也就是TaskTracker循环向JobTracker发送心跳，向上级报告自己这边是不是还活着，活干的怎么样了，可以接些新活等。作为JobTracker，接收到心跳信息，如果有待分配任务，就会给这个TaskTracker分配一个任务，然后taskTracker就把这个任务加入到他的任务队列中。我们可以主要看看TaskTracker中的transmitHeartBeart()和JobTracker的heartbeat()方法。 执行任务TaskTracker申请到任务后，在本地执行，主要有以下几个步骤来完成本地的步骤化: 将job.split复制到本地 将job.jar复制到本地 将job的配置信息写入到Job.xml 创建本地任务目录，解压job.rar 调用launchTaskForJob()方法发布任务 发布任务后，TaskRunner会启动新的java虚拟机来运行每个任务，以map任务为例，流程如下： 配置任务执行参数（获取java程序的执行环境和配置参数等） 在child临时文件表中添加Map任务信息 配置log文件夹，配置Map任务的执行环境和配置参数； 根据input split,生成RecordReader读取数据 为Map任务生成MapRunnable，一次从RecordReader中接收数据，并调用map函数进行处理 将Map函数的输出调用collect收集到MapOUtputBuffer中 2. MapReduce中排序应用实现过程Map阶段Read(读取) ==&gt; Collect(生成Key-Value) ==&gt; Spill(溢写) Read:从HDFS读取inputSplit（由InputFormat根据文件生成） Collect:分为map过程和partition过程，map根据inputSplit生成Key-Value对，而Partition添加分区标记（辅助排序用），并写入环形缓存区。 Spill:分为sort过程、compress过程以及combine过程。数据不断的写入环形缓存区，达到阈值之后开始溢写，在溢写的过程中进行一次Sort，这里使用的排序是快排（QuickSort）；一次溢出生成一个file，并且在生成file的过程中进行压缩（compress）；多个file又会进行一次文件合并，在文件合并的过程中进行排序，这里使用的排序是归并排序（MergeSort）。 Shuffle阶段Shuffle阶段主要就是一个数据拷贝的过程，Map端合成的大文件之后，通过HTTP服务(jetty server)拷贝到Reduce端。拷贝到Reduce端的数据并不是马上写入文件，而是同样放在缓存中，达到阈值则进行溢写。 Reduce阶段合并溢写生成的file，这里使用的排序为归并排序(MegerSort)，生成一些更大的文件(进一步减少文件个数)。在归并之后留下少量的大文件，最后对大文件进行一次最终合并，合并成一个有序的大文件(只有一个)，这里使用的排序算法为堆排序(HeapSort)。 总结综上所述，一个MapReduce过程涉及到了一次快排、两次归并以及一次堆排的操作。 参考资料Hadoop入门第三篇-MapReduce试手以及MR工作机制]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>方法优化</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神策数据实习]]></title>
    <url>%2Fpassages%2F2019-07-01-%E7%A5%9E%E7%AD%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1. 笔试拓扑排序：可以实现有向图以及无向图判断是否有环存在。 稳定排序算法：归并排序、基数排序、冒泡排序和直接插入排序 2. 面试时间很短，每人只有15分钟，本人大概面试了20分钟左右，面试官不是做算法的，技术能力很强（认识…），效果不理想，主要是自己菜—&gt;. 问题1：描述一下过拟合。 问题2：给一些数据，如何选取模型去挑选数据，判断与随机取数据的好坏。 问题3：分布式了解吗？MR工作机制？ 问题4：给文章数据让你统计词频，你怎么实现会有哪些问题？大量数据怎么处理？正常字典法请手撕代码。 问题5：针对于分词那么换行问题“hell-\nO world”如何处理，文章应当一部分一部分去处理，如果分句针对较长的句子该怎么办？（面试官不看好replace(“-\n”,””)）我也没有合适的解决方案…..]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典笔试——找到数组第k大或第k小的数]]></title>
    <url>%2Fpassages%2F2019-06-30-%E6%89%BE%E5%88%B0%E6%95%B0%E7%BB%84%E7%AC%ACk%E5%A4%A7%E6%88%96%E7%AC%ACk%E5%B0%8F%E7%9A%84%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. 问题在未排序的数组中找到第k个最大的元素，找到数组排序后的第k个最大的元素。 示例： 输入： 1[3,2,3,1,2,4,5,5,6] 和 k=4 输出：4 2. 解题思路类快速排序思想，找到数组中元素的位置，当分界点的索引为k-1的时候，它就是第k大元素，第k小的数只需找（组数长度+1-k）大的数即可。其时间复杂度应小于$O(n\log_2n)$。 123456789101112131415161718192021class Solution(object): def findKthLargest(self,nums,k): return self.findKthSmallest(nums,len(nums)+1-k) def findKthSmallest(self,nums,k): if nums: pos = self.partition(nums,0,len(nums)-1) if k&gt;pos+1: return self.findKthSmallest(nums[pos+1:],k-pos-1) elif k&lt;pos+1: return self.findKthSmallest(nums[:pos],k) else: return nums[pos] def partition(self,nums,l,r): low = l while l&lt;r: if nums[l] &lt; nums[r]: nums[l],nums[low] = nums[low],num[l] low += 1 l += 1 nums[low],nums[r] = nums[r],nums[low] return low 3. 其他解法堆排序方法]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>python</tag>
        <tag>必会题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(SVM)原理与推导]]></title>
    <url>%2Fpassages%2F2019-06-30-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM-%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[1. 支持向量机的原理Support Vector Machine (SVM)是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分隔超平面的线形分类器。（间隔最大是它有别于感知机），通过该超平面实现对未知样本集的分类。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机； 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机； 当训练数据线性不可分时，通过使用核技巧及间隔最大化，学习非线性支持向量机。 2. SVM推导线性分类器是最简单的有效分类器形式，SVM则在线性分类器基础上演化而成。 所谓线性函数，在一维空间中就是一个点，在二维空间中就是一条直线，三维空间就是一个平面，随着维度不断增加，线性函数也不断改变，被叫做超平面。 SVM的目标就是找到图中的超平面，在任意维空间中，超平面可以表示为函数：$$G(x)=W^TX+b$$一个样本点到$P(X_i,Y_i)$超平面的几何距离为：$$\frac{\left|W^TX_i+b\right|}{\left|{W}\right|}$$Note:这里的几何距离是距离而非间隔。 对于正样本$Y_i=1$，意味着该点在超平面正样本一侧：$$W^TX_i+b&gt;0$$对于负样本$Y_i =-1$，意味着该点在超平面负样本一侧：$$W^TX_i+b&lt;0$$所以可用下述公式表示点到超平面的几何距离：$$\frac{Y_i(W^TX_i+b)}{\left|W\right|}$$其中，分子部分表示函数间隔。 几何间隔可以表示点到超平面的距离，在进行分类过程中使分类更加准确，需要优化所有样本中到分类超平面几何间隔最小点的间隔值更大，所以对于SVM公式的求解转换为了一个最优化问题：$$\max_{W,b}[\min_{X_i}\frac{Y_i(W^TX_i+b)}{\left|W\right|}]$$即求解出最优的$W,b$使得到超平面最近的样本几何间隔最大。 对于一个超平面，如果等比例缩放$W,b$的值其公式表示的超平面不变，我们可以假设到超平面距离最小的样本点到超平面的函数间隔都大于1（通过缩放可$W,b$以实现）：$$Y_i(W^TX_i+b)\ge1$$由此可以将原最优化问题进行改变为带约束条件的最优化问题，优化目标为：$$\max_{W,b}\frac{1}{\left|W\right|}$$约束条件为：$$Y_i(W^TX_i+b)\ge1，i=1,2,3…k$$为了便于求解，目标函数等价于：$$\min_{W,b}\frac{1}{2}\left|w\right|^2$$下面便对该式子进行求解。对于没有约束的最优化问题，我们可以采用求导的方式求解出最优解。针对带约束最优化问题，一个比较好的方法是将其转化为没有约束条件的最优化问题，我们将带约束最优化问题泛化为：$$\min_xf(x)\\s.t.\\g(x)\le0,i=1,2,3…k\\h(x)=0,i=1,2,3…l$$泛化问题可以等价于求：$$\min_{x}\max_{\alpha,\beta}f(x)+\sum_{i=1}^k\alpha g_i(x)+\sum_{j=1}^l\beta h_j(x)$$其中$\alpha$大于零，$\beta$不等于零。若$g(x)$大于零，而$\alpha$也大于零，对于可变$\alpha$项则不存在极大值，关于$\beta$同理，其可正可负不为0，若$h(x)$不能满足等于零，在该项上也不存在极大值，通过max求极值实现了对原有约束条件的保留。 基于此SVM最优化方程转化为：$$\min_{W,b}\max_{\alpha}\frac{1}{2}\left|w\right|^2+\sum_{i=1}^k\alpha_{i}(1-Y_i(W^TX_i+b))\\s.t.\\\alpha_i\gt0,i=1,2,3…k$$在满足KTT条件的情况下满足拉格朗日对偶性，其等价于优化方程：$$\max_{\alpha}\min_{W,b}\frac{1}{2}\left|w\right|^2+\sum_{i=1}^k\alpha_{i}(1-Y_i(W^TX_i+b))\\s.t.\\\alpha_i\gt0,i=1,2,3…k\\1-Y_i(W^TX_i+b)\le0,i=1,2,3…k\\\alpha_{i}(1-Y_i(W^TX_i+b))=0,i=1,2,3…k$$对于该最优化的方程内部进行求导：$$\frac{\partial L}{\partial W} =W-\sum_{i=1}^k\alpha_{i}Y_iX_i\\\frac{\partial L}{\partial b} =-\sum_{i=1}^k\alpha_{i}Y_i$$在偏导数为0时取得极值。$$W= \sum_{i=1}^k\alpha_{i}Y_iX_i\\\sum_{i=1}^k\alpha_{i}Y_i=0$$将求导后的等式带入原方程可以得到最终关于$\alpha$的最优化方程：$$\max_{\alpha}\sum_{i=1}^k\alpha_i-\frac{1}{2}\sum_{i=1}^k\alpha_{i}Y_iX_i^T\sum_{j=1}^k\alpha_jY_jX_j\\s.t.\\\alpha_i\gt0,i=1,2,3…k\\1-Y_i(W^TX_i+b)\le0,i=1,2,3…k\\\alpha_{i}(1-Y_i(W^TX_i+b))=0,i=1,2,3…k\\\sum_{i=1}^k\alpha_iY_i=0$$求出该式子中各个$\alpha$的值，由于$W,b$与$\alpha$均有等价关系，便可以得到SVM模型的$W,b$参数。上面式子显然无法通过求导得到最优的$\alpha$值，可以使用Sequential Minimal Optimization(SMO)方法去进行求解。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2Fpassages%2F2019-06-30-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序1. 介绍快速排序是一种高效的排序算法，它采用“分而治之”的思想。其原理是：对于一组给定的记录，通过一趟排序后，将原序列分成两部分，其中前部分的所有记录均比后面部分的所有记录小，然后再依次对前后两部分的记录进行快速排序，递归该过程，直到序列中的所有记录均有序为止。 具体算法步骤如下： (1) 分解: 将输入的序列array[m,…,n]划分成两个非空子序列array[m,…,k]和array[k+1,…,n]，使array[m,..,k]和array[k+1,…,n]，使array[m,…,k]中任一元素的值不大于array[k+1,…,n]中任一元素的值。 (2) 递归求解：通过递归调用快速排序算法分别对array[m,…,k]和array[k+1,…,n]进行排序。 (3) 合并: 由于对分解出的两个子序列的排序是就地进行的，所以在array[m,…k]和array[k+1,…,n]都是排好序后，不需要执行任何计算，array[m,…,n]就已排好序。 2. Python代码实现123456789101112131415161718192021222324252627282930# 如何进行快速排序# 整体有序,基准左边或右边样本为空，比较次数多，最坏时间复杂度，例如基准最小，排序递增# 最坏时间复杂度为O(n^2),最好时间复杂度为O(nlogn),平均时间复杂度为O(nlogn)# 平均空间复杂度为O(nlogn)def quick_sort(left,right,lists): if left &gt; right: return lists key = lists[left] low = left high = right while left &lt; right: while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] = lists[right] while left &lt;right and lists[left] &lt;= key: left += 1 lists[right] = lists[left] lists[left] = key quick_sort(low,left-1,lists) quick_sort(left+1,high,lists) return listsif __name__ == "__main__": lists = [3,4,2,8,9,5,1] print("排序前序列为：",end="") for i in lists: print(i,end=" ") print("\n排序后序列为：",end="") for i in (quick_sort(0,len(lists)-1,lists)): print(i,end=" ") 3 参考资料 《Python程序员面试算法宝典》]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>排序算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Long and Short Term Memory（LSTM）的理解]]></title>
    <url>%2Fpassages%2F2019-06-29-Long-and-Short-Term-Memory%EF%BC%88LSTM%EF%BC%89%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. LSTM网络定义长短期记忆网络——通常被称为 LSTM，是一种特殊的 RNN，能够学习长期依赖性。由 Hochreiter 和 Schmidhuber（1997）提出的，并且在接下来的工作中被许多人改进和推广。LSTM 在各种各样的问题上表现非常出色，现在被广泛使用。 LSTM 被明确设计用来避免长期依赖性问题。长时间记住信息实际上是 LSTM 的默认行为，而不是需要努力学习的东西！ 所有递归神经网络都具有神经网络的链式重复模块。在标准的 RNN 中，这个重复模块具有非常简单的结构，只有单个 tanh 层。 LSTM 也具有这种类似的链式结构，但重复模块具有不同的结构。不是一个单独的神经网络层，而是四个，并且以非常特殊的方式进行交互。 2. LSTM的核心思想LSTM 的关键是细胞状态（cell state），即图中上方的水平线。它贯穿整个链条，只有一些次要的线性交互作用。信息很容易以不变的方式流过。LSTM可以通过“门”结构实现向细胞状态添加或移除信息的操作。 “门”结构通过一个 sigmoid 的神经层和一个逐点相乘的操作来实现信息选择通过。LSTM通过三个这样的门结构来实现信息的保护和控制。这三个门分别输入门（input gate）、遗忘门（forget gate）和输出门（output gate）。 2.1 遗忘门LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息，这一过程由遗忘门来完成，其过程如下图所示。 遗忘门会读取$h_{t-1}$和$x_t$，输出一个在 0到 1之间的数值给每个在细胞状态$C_{t-1}$中的数字。1表示“完全保留”，0 表示“完全舍弃”。 对于语言模型，若已知先前的词汇预测下一词汇，当前一刻细胞状态包括当前主语的性别，这信息需要保留下来帮助我们使用正确的代词。但当我们看到一个新的主语时，我们需要忘记先前的主语。 2.2 输入门下一步是决定让多少新的信息加入到细胞状态中来。实现这个需要包括两个步骤： 一个“输入门”的 sigmoid网络层确定哪些信息需要更新 一个 tanh 网络层创建一个新的备选值向量—— $ \hat C_t $可以用来添加到细胞状态 之后，我们把这两部分联合起来，对细胞状态进行一个更新，其过程如下图。 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。 2.3 输出门现在更新旧的细胞状态 $C_{t-1}$ 更新到 $C_t$。然后，我们对旧的状态乘 $f_t$，用来忘记我们决定忘记的事。然后我们加上 $i_t*\hat C_t$，这是新的候选值，根据我们对每个状态决定的更新值按比例进行缩放。 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪些部分将输出出去。然后，我们把细胞状态输入 $tanh$处理（把数值调整到 $−1$和 $1$ 之间）再和 sigmoid 网络层的输出相乘，这样我们就可以输出想要输出的部分。 同样，以语言模型为例子，一旦出现一个主语，主语的信息会影响到随后出现的动词。例如，知道主语是单数还是复数，就可以知道随后动词的形式。 参考资料 Stacked Long Short-Term Memory Networks Understanding LSTM Networks]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>神经网络</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法比较]]></title>
    <url>%2Fpassages%2F2019-06-29-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[各种排序算法比较 类别 排序方法 时间复杂度 空间复杂度 稳定性 平均情况 最好情况 最坏情况 辅助空间 插入排序 直接插入 O($n^2$) O($n$) O($n^2$) O($1$) 稳定 Shell排序 O($n^{1.3}$) O($n$) O($n^2$) O($1$) 不稳定 选择排序 直接选择 O($n^2$) O($n^2$) O($n^2$) O($1$) 不稳定 堆排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($1$) 不稳定 交换排序 冒泡排序 O($n^2$) O($n$) O($n^2$) O($1$) 稳定 快速排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n^2$) O($n\log_{2}{n}$) 不稳定 归并排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n$) 稳定 基数排序 O($d(r+n)$) O($d(n+rd)$) O($d(r+n)$) O($rd+n$) 稳定 Note:基数排序的复杂度中，r代表关键字的基数，d代表长度，n代表关键字的个数。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>排序算法</tag>
        <tag>复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo与github搭建个人博客]]></title>
    <url>%2Fpassages%2Fhexo%2F</url>
    <content type="text"><![CDATA[GitHub个人博客搭建主要有两种方法：一是基于Ruby的jekyll+github方法，二是基于Nodejs的Hexo+github方法。这是本人的第一篇个人博客，希望能够坚持写下去。下面我将介绍下Hexo+github方法的具体过程： 搭建过程Hexo安装12$ npm install hexo-cli -g$ npm install hexo-deployer-git --save 第一句是安装Hexo，第二句是安装Hexo部署到git page的deployer保证与github关联。 主题建站123$ cd your_blog_dir$ hexo init blog$ git clone https://github.com/dongyuanxin/theme-ad.git themes/ad 安装完成后，根据自己喜好建立目录。Hexo 将会在指定文件夹中新建所需要的文件。之后我们可以在Hexo官网上选取主题从Github上clone到本地的themes文件夹下。 针对于不同的主题，blog下的_config.yml需进行如下操作更换主题、与Github关联： 12345theme: 主题名deploy: type: git repository: git@github.com:nijunssdut/nijunssdut.github.io.git branch: master 在这里，我们配置Github的SSH密钥可以让本地git项目与远程的github建立联系，让我们在本地写了代码之后直接通过git操作就可以实现本地代码库与Github代码库同步。 12345$ cd ~/ .ssh$ ssh-keygen -t rsa -C "your_email@example.com"# 这将按照你提供的邮箱地址，创建一对密钥$ pbcopy &lt; ~/.ssh/id_rsa.pub# 将公钥的内容复制到系统粘贴板 之后在Github的Account Settings-SSH Keys中粘贴添加密钥即可。 更多信息详见: 参考博客 配置与测试这部分主要介绍Hexo命令的使用。 1234$ hexo clean$ hexo generate$ hexo server$ hexo deploy hexo clean与hexo generate一般一起使用清理并生成编写内容，执行完hexo server后可在本地使用https://localhost:4000 查看建站情况，hexo deploy会更新Github端个人博客的内容：Jun的个人主页 Markdown写作本人使用Mac OS，推荐Typora软件进行Markdown编写。Typora是一款轻便简洁的Markdown编辑器，支持即时渲染技术。 Typora语法相对简单，可参考简书typora、博客园typora。 Hexo博客内容图片显示问题12$ npm install eslint$ npm install hexo-asset-image --save 由于缺少eslint依赖直接安装Hexo图片插件会有警告。完成安装后用hexo新建文章的时候会发现_posts目录下面会多出一个和文章名字一样的文件夹。图片就可以放在文件夹下面。插入图片的方式采用Markdown语法即可。 Note:把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
