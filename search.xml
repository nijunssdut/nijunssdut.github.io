<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[朴素贝叶斯专题]]></title>
    <url>%2Fpassages%2F2019-07-24-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯算法“朴素”的含义朴素贝叶斯模型（Naive Bayesian Model）朴素的含义：建立在两个前提假设上： 特征之间相互独立 每个特征同等重要 然而这种属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果原因在于： （1）对于分类任务来说，只要各类别的条件概率排序正确、无需精准概率值即可导致正确分类； （2）如果属性间依赖对所有类别影响相同，或依赖关系的影响能够相互抵消，则属性条件独立性假设在降低计算开销的同时不会对性能产生负面影响 工作原理 假设现在有样本$x = (a_1,a_2,a_3,…a_n)$这个待分类项（并认为x里面是特征独立的） 再假设现在有分类目标$Y = {y_1,y_2,y_3,..y_n}$ 那么$\max(P(y_1|x),P(y_2|x),P(y_3|x)…,P(y_n|x))$就是最终的分类类别 $P(y_i|x) = \frac{P(x|y_i)*P(y_i)}{P(x)}$ 因为x对于每个分类目标来说都一样，所以就是求$\max(P(x|y_i)*P(y_i))$ $P(x|y_i)p(y_i)= p(y_i)\prod{P(a_i|y_i)}$ 而具体的$P(a_i|y_i)$和$P(y_i)$都是能从训练样本中统计出来 $P(a_i|y_i)$表示该类别下该特征出现的频率，$P(y_i)$表示全部类别中这个类别出现的概率。 工作流程 准备阶段 确定特征属性，并对每个特征属性适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。 训练阶段 计算每个类别在训练样本中的出现频率以及每个特征属性划分对每个类别的条件概率估计 应用阶段 使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别 朴素贝叶斯模型的种类朴素贝叶斯三个常用模型：高斯、多项式、伯努利 高斯模型主要处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度 多项式模型：$$P(x_i|y_k) = \frac{N_{y_kx_i}+\alpha}{N_{y_k}+\alpha n}$$其中$\alpha$是拉普拉斯平滑，加和的是属性出现的总次数，也防止了零概率问题。在文本分类问题中，反映一个词出现的词频，类似投骰子问题n次出现m次这个词的场景。 伯努利模型：伯努利模型特征的取值为布尔型，即出现为True，没有出现为False，在文本分类中，就是一个单词有没有在一个文档中出现。 朴素贝叶斯的应用场景 文本分类/垃圾文本过滤/情感判别 多分类实时预测 推荐系统 朴素贝叶斯和协同过滤是一对好搭档，协同过滤是强相关性，但泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。 贝叶斯决策理论将分类看做决策，进行贝叶斯决策时考虑各类的先验概率和类条件概率，即后验概率。考虑先验概率意味着对样本总体的认识，考虑类条件概率是对每一类中某个特征出现频率的认识。由此不难发现，贝叶斯决策的理论依据就是贝叶斯公式。 最小错误率贝叶斯决策贝叶斯决策的基本理论依据就是贝叶斯公式，判决遵从最大后验概率。这种仅根据后验概率作决策的方式称为最小错误率贝叶斯决策，可以从理论上证明这种决策的平均错误率是最低的。 最小风险贝叶斯决策另一种方式是考虑决策风险，加入了损失函数，称为最小风险贝叶斯决策。 朴素贝叶斯三种常用的分类模型朴素贝叶斯的三个常用模型：高斯、多项式、伯努利。 高斯模型主要处理包含连续型变量的数据，使用高斯分布概率密度来计算类的条件概率密度； 多项式模型：$$P(x_i|y_k) = \frac{N_{y_kx_i}+\alpha}{N_{y_k}+\alpha n}$$其中$\alpha$是拉普拉斯平滑，加和的是属性出现的总次数，也防止了零概率问题。在文本分类问题中，反映一个词出现的词频，类似投骰子问题n次出现m次这个点数的场景。 伯努利模型： 伯努利模型特征的取值为布尔型，即出现为true，没有出现为false，在文本分类中，就是一个单词有没有在一个文档中出现。 朴素贝叶斯细节问题零概率问题描述：在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果为0。 解决方案：通常解决这个问题的方法是要进行平滑处理，常用拉普拉斯修正。 拉普拉斯修正的含义是，在训练集中总共的分类数，用 N 表示；di 属性可能的取值数用 $N_i$ 表示，因此， 原来的先验概率$P(c)$的计算公式由：$P(c) = \frac{D_c}{D}$ 被拉普拉斯修正为：$P(c) = \frac{D_c+1}{D+N}$ 类的条件概率$P(x|c)$的计算公式由：$P(x_i|c) = \frac{D_{c,st}}{D_c}$ 被拉普拉斯修正为：$P(x_i|c) = \frac{D_{c,st}+1}{D_c+N_i}$ 使用拉普拉斯平滑，拉普拉斯因子的大小如何确定？朴素贝叶斯中的拉普拉斯因子$\alpha$无法通过公式求出最优大小，需要根据程序员的经验去设置，使用交叉验证的方式求取最优大小。 下溢问题问题描述：在计算过程中，需要对特定分类中各个特征出现的概率进行连乘，小数相乘，越乘越小，造成下溢出，计算结果变成0。 解决方案：通过log运算增大概率的绝对值。log运算不会影响函数的趋势和极值只是扩大值得范围。将小数的乘法操作转化为取对数后的加法操作，规避了变为零的风险同时并不影响分类结果。 异常值敏感问题朴素贝叶斯是一种对异常值不敏感的分类器，保留数据中的异常值，常常可以保持贝叶斯算法的整体精度，如果对原始数据进行降噪训练，分类器可能会因为失去部分异常值的信息而导致泛化能力下降。 异常值不敏感原因：可能是因为朴素贝叶斯分类器是一个概率模型，如果输入是一个正常的样本，则异常值并不会影响到正常样本的后验概率。因为对于正常样本而言$p(x|y_i)*p(y_i)=p(y_i)\prod_j{p(x_j|y_i)}$，其中$x_j$是正常的，并不会使用到异常值。如果是一个异常的$p(x_j|y_i)$，反而已有的异常值可以帮助到该异常样本更好的分类。 异常值有影响的情况：如果是对于连续型属性的异常值则会产生对分类器产生一定的影响，因贝叶斯对连续值的处理往往是通过估计其概率分布的参数，若有异常值存在则其概率分布将会产生偏移。若是分类变量则之间统计出现次数是不会产生偏移的。 缺失值敏感问题不敏感 原因：朴素贝叶斯算法能够处理缺失的数据，在算法的建模时和预测时数据的属性都是单独处理的。因此如果一个数据实例缺失了一个属性的数值，在建模时将被忽略，不影响类条件概率的计算，在预测时，计算数据实例是否属于某类的概率时也将忽略缺失属性，不影响最终结果。 数据的属性是连续型变量的情况当朴素贝叶斯算法数据的属性为连续型变量时，有两种方法可以计算属性的类条件概率。 第一种方法是把一个连续的属性离散化，然后用相应的离散区间替换连续属性值，之后用频率去表示类条件概率。但这种方法不好控制离散区间划分的粒度。如果粒度太细，就会因为每个区间内训练记录太少而不能对做出可靠估计，如果粒度太粗，那么有些区间就会有来自不同类的记录，因此失去了正确的决策边界。 第二种方法是假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，例如可以使用高斯分布来表示连续属性的类条件概率分布。$$P(X_i = x_i|Y=y_i) =\frac{1}{\sqrt{2\pi\sigma_{i,j}}}e^{-\frac{(x_i-\mu_{i,j})^2}{2\sigma_{i,j}^2}}$$其中$\mu_{i,j}$为类$y_j$的所有训练记录关于$X_i$的样本均值估计，$\sigma_{i,j}^2$为类$y_i$的所有训练记录关于$X_i$的样本方差估计。通过高斯分布估计出类条件概率。 高度相关的特征对朴素贝叶斯的影响假设有两个特征高度相关，相当于该特征在模型中发挥了两次作用（计算两次条件概率），使得朴素贝叶斯获得的结果向该特征所希望的方向进行了偏移，影响了最终结果的准确性，所以朴素贝叶斯算法应先处理特征，把相关特征去掉。 朴素贝叶斯的增量计算传统的贝叶斯方法在有新的训练样本加入时，需要重新学习已经学习过的样本，耗费大量时间。增量计算就是在原有分类器的基础之上，自主选择学习新的文本来修正分类器。因为朴素贝叶斯在训练过程中实际只需要计算出各个类别的概率（先验）和各个特征的类条件概率，这些概率值可以快速的根据增量数据进行更新，无需重新全量训练，所以其十分适合增量计算，该特性可以使用在超出内存的大量数据计算和随时间等（流数据）获取数据的计算中。 朴素贝叶斯总结朴素贝叶斯是高偏差低方差在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为Error = Bias+Variance。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望（平均值）之间的误差，即模型的稳定性，数据是否集中。 对于朴素贝叶斯，它简单的假设了各个数据之间是无关的，是一个被严重简化了的模型，对于复杂模型，充分拟合了部分数据，使得他们的偏差较小，而由于对部分数据的过度拟合，对于部分数据预测效果不好，整体来看可能引起方差较大，简单模型与之相反，大部分场合偏差部分大于方差部分，也就是高偏差低方差。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 朴素贝叶斯的优缺点优点： 对数据的训练快，分类也快 对缺失数据不太敏感，对异常值也不太敏感，算法也比较简单 对小规模的数据表现很好，能够处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练 缺点： 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。 对输入数据的表达形式很敏感。（离散的类别之间统计频率即可，连续值就要估计概率分布。） 参考资料 机器学习面试题之NB——朴素贝叶斯]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
        <tag>贝叶斯网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率图模型概述]]></title>
    <url>%2Fpassages%2F2019-07-23-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[概率图概率图中的节点分为隐含节点和观测节点，边分为有向边和无向边。从概率论的角度，节点对应于随机变量，边对应于随机变量的依赖或相关关系，其中有向边表示单向的依赖，无向边表示相互依赖关系。 概率图模型分为贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）两大类。贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。更详细地说，概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在机器学习的诸多场景中都有着广泛的应用。 马尔可夫网络的联合概率分布在马尔可夫网络中，联合概率分布的定义为$$P(x) = \frac{1}{Z}\prod_{Q \in C}\varphi_{Q}(X_Q)$$其中$C$为图中最大团所构成的集合，$Z = \sum_{x} \prod_{Q \in C}\varphi_{Q}(X_Q)$为归一化因子，用来保证$P(x)$是被正确定义的概率，$\varphi_{Q}$是与团$Q$对应的势函数。势函数是非负的，并且应该在概率较大的变量上取得较大的值，例如指数函数$$\varphi_{Q}(X_Q)=e^{-H_{Q}(x_Q)}$$其中，$$H_Q(x_Q)= \sum_{\mu,v \in Q,\mu \neq v}\alpha_{\mu,v}x_{\mu}x_v+\sum_{v \in Q}\beta_vx_v$$对于图中所有节点$x={x_1,x_2,…,x_n}$所构成的一个子集，如果在这个子集中，任意两点之间都存在边相连，则这个子集中的所有节点构成了一个团。如果在这个子集中加入任意其他节点，都不能构成一个团，则称这样的子集构成了一个最大团。 对于由A，B，C，D四个点构成的四边形无向图，其联合概率分布为：$$P(A,B,C,D)=\frac{1}{Z}e^{-H(A,B,C,D)}$$ 概率图表示解释朴素贝叶斯模型原理朴素贝叶斯模型通过预测指定样本属于特定类别的概率$P(y_i|x)$来预测该样本的所属类别，即$$y = \max_{y_i}P(y_i|x)$$$P(y_i|x)$可以写成$$P(y_i|x) = \frac{P(x|y_i)P(y_i)}{P(x)}$$其中$x= (x_1,x_2,x_3,…,x_n)$为样本对应的特征向量，$P(x)$为样本的先验概率。对于特定的样本x和任意类别$y_i$，$P(x)$的取值均相同，在计算中可以被忽略。假设特征相互独立，$P(y_i)$可以通过训练样本统计得到，后验概率$P(x_j|y_i)$的取值决定了分类的结果。并且任意特征$x_j$都由$y_i$的取值所影响。 解释最大熵模型的原理信息是指人们对事物理解的不确定性的降低或消除，而熵是不确定性的度量，熵越大，不确定性也就越大。 最大熵原理是概率模型学习的一个准则，指导思想是在满足约束条件的模型集合中选取熵最大的模型，即不确定性最大的模型。 在对训练数据集一无所知的情况下，最大熵模型认为$P(y|x)$是符合均匀分布的。 给定离散随机变量$x$和$y$上的条件概率分布$P(x|y)$，定义在条件概率分布上的条件熵为：$$H(p) = -\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)$$其中$\tilde{P}(x)$为样本在训练数据集上的经验分布，即$x$的各个取值在样本中出现的频率统计。 当我们有了训练数据集之后，我们希望从中找到一些规律，从而消除一些不确定性，这时就需要用到特征函数$f(x,y)$。特征函数$f$描述了输入$x$和输出$y$之间的一个规律。为了使学习到的模型能够正确捕捉训练数据集中的特征，我们加入一个约束，使得特征函数$f(x,y)$关于经验分布$\tilde{P}(x,y)$的期望值与关于模型$P(y|x)$和经验分布$\tilde{P}(x)$的期望值相等。$$E_{\tilde{P}}(f) = \sum_{x,y}\tilde{P}(x,y)f(x,y)$$ $$E_p(f) = \sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$ 综上，给定训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，以及M个特征函数${f_i(x,y),i=1,2,…,M}$，最大熵模型的学习等价于约束最优化问题：$$\max_{p}H(P) = -\sum_{x,y}\tilde{P}(x)P(y|x)\log P(y|x)\ s.t., E_{\tilde{P}}(f) = E_p(f), \forall i =1,2,…,M,\ \sum_{y}P(y|x) = 1$$求解之后可以得到最大熵模型的表达形式为$$P_w(y|x) = \frac{1}{Z}exp(\sum_{i=1}^{M}w_if_i(x,y))$$最终，最大熵模型归结为学习最佳的参数$w$，使得$P_w(y|x)$最大化。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概率图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯混合模型]]></title>
    <url>%2Fpassages%2F2019-07-22-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[高斯混合模型高斯混合模型（Gaussian Mixed Model，GMM）是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。高斯混合模型假设每个簇的数据都符合高斯分布（又叫正态分布）。 高斯混合模型的核心思想高斯混合模型的核心思想是，假设数据可以看作多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，其均值$\mu_i$和方差$\sum_i$是待估计的参数。此外，每个分模型都还有一个参数$\pi_i$，可以理解为权重或生成数据的概率。高斯混合模型的公式为：$$p(x) = \sum_{i=1}^K\pi_iN(x|\mu_i,\sum_i)$$ 高斯混合模型与K均值算法的异同高斯混合模型与K均值算法的相同点是，它们都是可用于聚类算法；都需要指定K值；都是使用EM算法来求解；都往往只能收敛于局部最优。而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>面试笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K均值聚类]]></title>
    <url>%2Fpassages%2F2019-07-22-k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[K均值的基本思想k均值是最基础和最常用的聚类算法。它的基本思想是通过迭代方式寻找K个簇（cluster）的一种划分方案，使得聚类结果对应的代价函数最小。代价函数可以定义为各个样本距离所属簇中心点的误差平方和：$$J(c,\mu) = \sum_{i=1}^{M}||x_i-\mu_{c_i}||^2$$其中$x_i$代表第i个样本，$c_i$是$x_i$所属于的簇，$\mu_{c_i}$代表簇对应的中心点，$M$是样本总数。 K均值算法的优缺点是什么？如何对其进行调优？k均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类等。但是瑕不掩瑜，k均值聚类的优点也很明显：主要体现在：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是O(NKt)近乎线性，其中N是数据对象的数目，K是聚类的簇数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。 K均值算法的调优一般可以从以下几个角度出发。 （1）数据归一化和离群点处理 K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法之前通常需要对数据做预处理。 （2）合理选择K值 K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。K值的选择一般基于经验和多次实验结果。例如采用手肘法，K值越大，距离和越小。手肘法认为拐点就是K的最佳值。 了解：Gap Statistic算法 （3）采用核函数 传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，并在新的特征空间中进行聚类。 针对K均值算法的缺点，有哪些改进的模型？K均值算法的主要缺点： （1） 需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。 （2） K均值只能收敛到局部最优，效果受到初始值影响很大。 （3） 易受到噪点的影响。 （4） 样本点只能被划分到单一的类中。 K-means++算法： 在原始K均值算法的随机选择聚集中心的基础上改进，后面过程中都是一样的。K-means++按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心（0&lt;n&lt;K），则在选择第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。 ISODATA算法： 当K值的大小不确定时，可以使用ISODATA算法。当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA针对这个问题进行了改进。ISODATA的全称为迭代自组织数据分析法。 ISODATA算法在K均值算法的基础之上增加了两个操作：一是分裂操作，对应着增加聚类中心数；二是合并操作，对应减少聚类中心数。其缺点时需要指定的参数比较多。ISODATA算法的各个输入参数： （1）预期的聚类中心数目$K_o$。最终输出的聚类中心数目常见范围是从$K_o$的一半，到两倍$K_o$。 （2）每个类所要求的最少样本数目$N_{min}$。如果分裂后会导致某个子类别包含样本数目小于该阈值，就不会对该类别进行分裂操作。 （3）最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足（2），进行分裂操作。 （4）两个聚类中心之间所允许最小距离$D_{min}。$如果两个类靠的非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，则对两个类进行合并操作。 如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。 最大期望算法（EM算法）K均值聚类的迭代算法实际上是一种最大期望算法。简称EM算法。EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。由于隐变量是未知的，无法直接通过最大似然估计求解参数，这时需要利用EM算法来求解。 EM算法框架总结如下，由以下两个步骤交替进行直到收敛： （1）E步骤：计算隐变量的期望$$Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)},\theta)$$（2） M步骤：最大化，目的是通过最大化这个下界可以使得待优化函数向更好的方向改进。$$\theta = argmax\sum_{i=1}^{m}\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)}| \theta)}{Q_i(z^{(i)})}$$K均值算法等价于用EM算法求解含隐变量的最大似然问题。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec知识简述]]></title>
    <url>%2Fpassages%2F2019-07-08-Word2Vec%2F</url>
    <content type="text"><![CDATA[背景谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别是CBOW（Continues Bag of Words）和Skip-gram。 负采样（Negative Sample）和层次softmax（Hierarchical Softmax）则是两种加速训练的方法。 工作流程CBOW的目标是根据上下文出现的词语来预测当前词的生成概率；而Skip-gram是根据当前词来预测上下文中各词的生成概率。 其中$w(t)$是当前所关注的词，$w(t-2)、w(t-1)、w(t+1)、w(t+2)$是上下文中出现的词。这里前后滑动窗口大小均设为2。 CBOW和Skip-gram都可以表示成由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络。 输入层中的每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。 在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由N维输入向量以及连接输入和隐含单元之间的N*K维权重矩阵计算得到。在CBOW中，还需要将各个输入词所计算出的隐含单元求和。 同理，输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的K*N维权重矩阵计算得到。输出层也是一个N维向量，每维与词汇表中的一个单词相对应。最后，对输出层向量应用softmax激活函数，可以计算出每个单词的生成概率。Softmax函数定义为：$$p(y=w_n|x) = \frac{e^{x_{n}}}{\sum_{k=1}^{N}e^{x_{k}}}$$其中$x$代表N维的原始输出向量，$x_n$为在原始输出向量中，与单词$w_n$所对应维度的取值。 接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。从输入层到隐含层需要一个维度为N*K的权重矩阵，从隐含层到输入层需要一个维度K*N的权重矩阵，学习权重可以用反向传播算法实现。但由于Softmax中存在归一化项的缘故，推导出的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，因此需要改进。最终训练得到维度为N*K和K*N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。 负采样最后的softmax分出来的类是整个词袋的大小，那么是不是可以把词袋大小减小，因为有些出现概率低的词我们根本可以不考虑。这就是负采样的核心思想。 假设我们有一个训练样本，中心词是$w$，它周围上下文共有$2c$个词，记为$context(w)$。由于这个中心词$w$的确和$content(w)$相关存在，因此它是一个真实的正例。通过Negative Sampling 采样，得到n个和$w$不同的中心词$w_i,i =1,2,…,n$，这样$context(w)$和$w_i$就组成了n个并不存在的负例。利用这一个正例和n个负例，进行二元逻辑回归，得到负采样对应的每个词$w_i$对应的模型参数$\theta_i$和每个词的词向量。整个过程相比Hierarchical Softmax简单。 但需要明确两个问题：（1）如何通过一个正例和n个负例进行二元逻辑回归？（2）如何进行负采样？ 梯度计算这里我们将正例定义为$w_0$。 在逻辑回归中，正例期望满足：$$P(context(w_0),w_i)=\sigma(x_{w_0}^T\theta^{w_i}),y_i=1,i=0$$负期望满足：$$P(context(w_0),w_i)=1-\sigma(x_{w_0}^T\theta^{w_i}),y_i=0,i=1,2,..n$$其对应的对数似然函数为：$$L= \sum_{i=0}^ny_ilog(\sigma(x_{w_0}^T\theta^{w_i}))+(1-y_i)log(1-\sigma(x_{w_0}^T\theta^{w_i}))$$和层次Softmax类似，采用随机梯度上升法，仅仅每次只用一个样本更新梯度，来进行迭代更新得到我们需要的$x_{w_i},\theta^{w_i}$。求导得梯度表达式，用梯度上升法进行一步步的求解。 负采样方法word2vec采用的方法不复杂，如果词汇表的大小为V，那么我们就将一段长度为1的线段分成V份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短，每个词$w$的线段长度由下式决定：$$len(w) =\frac{count(w)}{\sum_{\mu\in vocab}count(\mu)}$$在word2vec中，分子和分母都取了3/4次幂如下：$$len(w) =\frac{count(w)^{3/4}}{\sum_{\mu\in vocab}count(\mu)^{3/4}}$$ 在采样前，我们将这段长度为1的线段划分成$M$等份，这里$M&gt;&gt;V$，这样可以保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从$M$个位置中采样出$n$个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。 在word2vec中，$M$取值默认为$10^8$ 层次softmaxHierarchical Softmax，层次softmax是一种加速训练的技巧，要解决的问题是原来softmax的参数太多的问题，所以不管是层次softmax也好，还是负采样也好，都是对最后的softmax做一个处理。 层次softmax的核心内容是哈夫曼树（Huffman Tree）。树的核心概念是 出现概率越高的符号使用较短的编码（层次越浅），出现概率低的符号则使用较长的编码（层次越深）。 缺点使用霍夫曼树来替代传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词$w$是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。负采样比之而言考虑了该问题，不再使用霍夫曼树。 参考资料 《百面机器学习》 word2vector的原理，结构，训练过程 word2vec原理]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>NLP</tag>
        <tag>Word2Vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程知识点整理概述(二)]]></title>
    <url>%2Fpassages%2F2019-07-07-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%E6%A6%82%E8%BF%B0-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[文本表示文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。 词袋模型（Bag of Words）最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重。TF-IDF内容详见我的博客：TF-IDF算法介绍 将文章进行单词级别的划分有时候并不是一种好的做法，比如英文中的natural language processing（自然语言处理）一词，如果将natural，language，processing这三个词拆分开来，所表达的含义与三个词连续出现时大相径庭。通常，可以将连续出现的n个词$(n\le N)$组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。另外，同一个词可能有多种词性变化，却具有相似的含义。在实际应用中，一般会对单词进行词干抽取（word Stemming）处理，即将不同词性的单词统一成为同一词干形式。 主题模型（Topic Model）主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。具体处理方法在后续的博文中楼主会详细介绍…… 词嵌入模型 （Word Embedding）词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K = 50~300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>NLP</tag>
        <tag>文本表示</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程知识点整理概述（一）]]></title>
    <url>%2Fpassages%2F2019-07-06-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[特征归一化为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。 数值类型特征常用归一化方法最常用的方法主要有以下两种： （1）线性函数归一化 (Min-Max Scaling) 它对原始数据进行线性变换，使结果映射到$[0,1]$的范围，实现对原始数据的等比缩放。归一化公式如下：$$X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}$$其中$X$为原始数据，$X_{max}、X_{min}$分别为数据最大值和最小值。 （2）零均值归一化（Z-Score Normalization） 它将原始数据映射到均值为0、标准差为1的分布上。具体来说，假设原始特征的均值为$\mu$、标准差为$\alpha$的分布上，那么归一化公式定义为：$$z = \frac{x-\mu}{\sigma}$$数据归一化不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。但对于决策树模型则并不适用。 类别型特征处理办法类别型特征原始输入通常是字符串形式，除了决策树等少数模型能够直接处理字符串形式，对于逻辑回归、支持向量机等模型来说，类别型的特征必须经过处理转换成数值型特征才能正确工作。 常用的类别型特征处理方法如下： 序号编码序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三档，并且“高&gt;中&gt;低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID。 独热编码独热编码通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用独热编码需要注意以下问题。 使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1，其他位置均取值为0。因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。 二进制编码二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。 其他编码方式还有很多，比如Helmert Constrast、Sum Contrast、Polynomial Contrast、Backward Difference Contrast等。 组合特征组合特征定义为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。 以逻辑回归为例，假设数据的向量特征为$X = (x_1,x_2,…,x_k)$，则有$$Y = sigmoid(\sum_i\sum_yw_{ij}&lt;x_i,x_j&gt;)$$其中$&lt;x_i,x_j&gt;$表示$x_i$和$x_j$的组合特征，$w_{ij}$的维度等于$|x_i|\cdot|x_j|$，$|x_i|$和$|x_j|$分别代表第$i$个特征和第$j$个特征不同取值的个数。 对于推荐算法，若用户数量为$m$，物品数量为$n$，那么需要学习参数的规模为$m*n$。在互联网环境下，用户数量和物品数量都可以达到千万量级，几乎无法学习$m*n$规模的参数。在这种情况下，一种行之有效的方法是将用户和物品分别用k维的低维向量表示$(k&lt;&lt;m,k&lt;&lt;n)$$$Y = sigmoid(\sum_i\sum_yw_{ij}&lt;x_i,x_j&gt;)$$这里$w_{ij}=x_{i}^{‘} \cdot x_{j}^{‘}$，$x_{i}^{‘}$和$x_{j}^{‘}$分别表示$x_i$和$x_j$ 对应的低维向量。那么在推荐问题中，需要学习的参数的规模变为$m*k+n*k$，这其实等价于矩阵分解。这就是利用降维方法来减少两个高维特征组合后需要学习的参数。 怎样有效地找到组合特征实际问题中并不是所有的特征组合都是有意义的。这里介绍一种基于决策树的特征组合寻找方法。于是，每一条从根节点到叶节点的路径都可以看成一种特征组合的方式。假设我们有4种特征组合方式，如果样本满足1，2特征组合方式，则其特征表示可以表示为$(1,1,0,0)$。 对于利用原始数据有效的构造决策树，我们可以采用基于boosting的GBDT梯度提升决策树，该方法的思想是每次都在之前构造的决策树的残差上构建下一棵决策树。 参考资料 《百面机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>特征工程</tag>
        <tag>归一化</tag>
        <tag>预处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF算法介绍]]></title>
    <url>%2Fpassages%2F2019-07-05-TF-IDF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[TF-IDF定义TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种加权技术。采用一种统计方法，根据字词在文本中出现的次数和在整个语料中出现的文档频率来计算一个字词在整个语料中的重要程度。 TF-IDF的主要思想TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF是词频(Term Frequency)词频（TF）表示词条（关键字）在文本中出现的频率。 这个数字通常会被归一化（一般是词频除以文章总词数），以防止它偏向长的文件。其公式如下：$$tf_{ij}=\frac{n_{i,j}}{\sum_kn_{k,j}}$$ $$TF_w=\frac{在某一类中词条w出现的次数}{该类中所有的词条数目}$$ 其中$n_{i,j}$是该词在文件$d_j$中出现的次数，分母则是文件$d_j$中所有词汇出现的次数总和。 IDF是逆向文件频率(Inverse Document Frequency)逆向文件频率（IDF）：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。 如果包含词条$t$的文档越少，IDF越大，则说明词条具有很好的类别区分能力。其公式如下：$$idf_{i}=\log{\frac{|D|}{|{j:t_i\in d_j}|}}$$其中，$|D|$ 是语料库中的文件总数。 $|{j:t_i\in d_j}|$表示包含词语 $t_i$ 的文件数目（即 $n_{i,j}≠0$ 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用$1+|{j:t_i\in d_j}|$ ，即：$$IDF = \log(\frac{语料库的文档总数}{包括词条w的文档数+1})$$ TF-IDF实际上是：TF * IDF某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。$$TF-IDF = TF*IDF$$Note：TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。 TF-IDF的应用 搜索引擎 关键词提取 文本相似性 文本摘要 python sklearn工具包下的TF-IDF调用 1class sklearn.feature_extraction.text.TfidfTransformer(norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False) norm：词向量归一化 参考资料 TF-IDF算法介绍及实现 文本特征提取之TFIDF与Word2Vec]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>文本表示</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划]]></title>
    <url>%2Fpassages%2F2019-07-04-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[定义动态规划与分治方法相似，都是通过组合子问题的解来求解原问题。分治方法将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解组合起来，求出原问题的解。与之相反，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题（子问题的求解是递归进行的，将其划分为更小的子子问题）。动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，避免了这种不必要的计算工作。 动态规划方法通常用来求解最优化问题。这类问题可以有很多可行解，每个解都有一个值，我们希望寻找具有最优值的解。我们称这样的解为问题的一个最优解，因为可能有多个解都达到最优值。 设计方法通常按如下4个步骤来设计一个动态规划算法： 刻画一个最优解的结构特征。 递归地定义最优解的值。 计算最优解的值，采用自底向上的方法。 利用计算出的信息构造一个最优解。 经典题目——最长公共子序列问题(LCS)与最长公共子串问题最长公共子序列问题（LCS问题）给定两个字符串A和B，长度分别为m和n，要求找出它们最长的公共子序列，并返回其长度。例如： A = "HelloWorld" B="loop" 则A与B的最长公共子序列为 “loo”,返回的长度为3。此处只给出动态规划的解法：定义子问题$dp[i][j]$为字符串A的第一个字符到第 i 个字符串和字符串B的第一个字符到第 j 个字符的最长公共子序列。 为了求解$dp[i][j]$，我们要先判断A的第i个元素B的第j个元素是否相同即判断A[i - 1]和 B[j -1]是否相同，如果相同它就是$dp[i-1][j-1]+ 1$，相当于在两个字符串都去掉一个字符时的最长公共子序列再加 1；否则最长公共子序列取$dp[i][j-1]$和$dp[i-1][j]$中大的那个。 问题的初始状态为：$$dp[i][0] = 0,dp[0][j] = 0$$相应的状态转移方程为：$$dp[i][j] =\begin{cases}\max{(dp[i-1][j],dp[i][j-1])}&amp; A[i-1]!=B[j-1]\\dp[i-1][j-1]+1&amp;A[i-1] == B[j-1]\end{cases}$$ 代码实现如下： 123456789101112131415161718def LongestSubstring(str1,str2): res = 0 str1_len = len(str1) str2_len = len(str2) if str1_len == 0 or str2_len == 0: return res dp = [[0 for j in range(str1_len)] for i in range(str1_len)] for i in range(str1_len): for j in range(str2_len): if str1[i] == str2[j]: if i == 0 or j == 0: dp[i][j] = 1 else: dp[i][j] = dp[i-1][j-1]+1 else: dp[i][j] = max(dp[i-1][j],dp[i][j-1]) return dp[i][j] 该算法的时间复杂度为$O(n*m)$，空间复杂度为$O(n*m)$。 Note：需要考虑空串的判定条件 最长公共子串问题给定两个字符串A和B，长度分别为m和n，要求找出它们最长的公共子串，并返回其长度。例如： A = "HelloWorld" B = "loop" 则A与B的最长公共子串为 "lo",返回的长度为2。我们可以看到子序列和子串的区别：**子序列和子串都是字符集合的子集，但是子序列不一定连续，但是子串一定是连续的**。 整个问题的初始状态为：$$dp[i][0] = 0,dp[0][j] = 0$$相应的状态转移方程为：$$dp[i][j] =\begin{cases}0&amp; A[i-1] !=B[j-1]\\dp[i-1][j-1]+1 &amp; A[i-1] == B[j-1]\end{cases}$$代码实现如下： 1234567891011121314151617def lcsubstr(str1,str2): res = 0 str1_len = len(str1) str2_len = len(str2) if str1_len == 0 or str2_len == 0: return res dp = [[0 for j in range(str1_len)] for i in range(str1_len)] for i in range(str1_len): for j in range(str2_len): if str1[i] == str2[j]: if i == 0 or j == 0: dp[i][j] = 1 else: dp[i][j] = dp[i-1][j-1]+1 if dp[i][j]&gt; res: res = dp[i][j] return res 该算法的时间复杂度为$O(n*m)$ ，空间复杂度为$O(n*m)$。 Note：需要考虑空串的判定条件 参考资料 《算法导论第三版》]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>经典算法</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制整理]]></title>
    <url>%2Fpassages%2F2019-07-03-Attention%E6%9C%BA%E5%88%B6%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Attention的定义与作用按照Stanford大学课件上的描述，attention的通用定义如下： 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。 attention的重点就是这个集合values中的每个value的“权值”的计算方法。 有时候也把这种attention的机制叫做query的输出关注了原文的不同部分。（Query attends to the values） 换句话说，attention机制就是一种根据某些规则或者某些额外信息（query）从向量表达集合（values）中抽取特定的向量进行加权组合（attention）的方法。简单来讲，只要我们从部分向量里面搞了加权求和，那就算用了attention。 Attention-based Model其实就是一个相似性的度量，当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入。 参考资料 浅谈attention机制 自然语言处理中的Attention机制总结]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[作业帮算法卷笔试]]></title>
    <url>%2Fpassages%2F2019-07-02-%E4%BD%9C%E4%B8%9A%E5%B8%AE%E7%AE%97%E6%B3%95%E5%8D%B7%E7%AC%94%E8%AF%95%2F</url>
    <content type="text"><![CDATA[博主问题笔记不包含博主经历所有题目只记录典型问题（估计也差不多全部了，因为博主菜…..） 问题1：交叉熵公式 解答：交叉熵公式如下：$$H(x,y) = -\sum_{i=1}^nx_i\ln{y_i}$$这里公式定义，x、y都是表示概率分布。其中x是正确的概率分布，而y是我们预测出来的概率分布，这个公式算出来的结果，表示y与正确答案x之间的错误程度（即：y错得有多离谱），结果值越小，表示y越准确，与x越接近。 问题2：随机森林与GDBT的异同 解答： 理论知识随机森林随机森林是一个用随机方式建立的，包括多个决策树的集成分类器。其输出的类别由各个树投票而定（如果是回归树则取平均）。假设样本总数为n，每个样本的特征数为a。则随机森林生成过程如下： 从原始样本中采用有放回抽样的方法选取n个样本; 对n个样本选取a个特征中的随机k个，用建立决策树的方法获得最佳分割点； 重复m次，获取m个决策树； 对输入样例进行预测时，每个子树都产生一个结果，采用多数投票机制输出。 随机森林的随机性主要体现在两个方面： 数据集的随机选取：从原始的数据集中采取有放回的抽样(bagging)，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。 待选特征的随机选取：与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选择最优的特征。 随机森林的优点： 实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的； 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合； 能够处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的； 对于不平衡的数据集，可以平衡误差； 相比SVM，不是很怕特征缺失，因为待选特征也是随机选取； 训练完成后可以给出哪些特征比较重要。 随机森林的缺点： 在噪声过大的分类和回归问题还是容易过拟合； 相比于单一决策树，它的随机性让我们难以对模型进行解释。 GBDTGBDT （Gradient Boost Decision Tree）是以决策树为学习器迭代算法，注意GDBT里的决策树都是回归树而不是分类树。Boost是“提升”的意思，一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。 GBDT的核心就在于：每一棵树学的是之前所有树结果和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学习。GBDT优点是适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。缺点是由于弱分类器的串行依赖，导致难以并行训练数据。 随机森林和GBDT的区别 随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回均匀取样，而boosting根据错误率来取样（Boosting 初始化时对每一个训练样例赋相等的权重$\frac{1}{n}$，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样本赋以较大的权重），因此Boosting的分类京都要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而boosting的训练集的选择与前一轮的学习结果有关，是串行的。 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。 组成随机森林的树可以并行生成；而GBDT只能是串行生成。 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。 随机森林对异常值不敏感；GDBT对异常值非常敏感。 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能。 参考资料：机器学习：随机森林和GDBT的区别 问题3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于$P(B) =\frac{P(AB)}{P(A)}$求后验概率问题。 问题4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为$O(d(r+n))$被忽略其线性。 问题5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题6：剑指Offer原题，螺旋遍历 解答：主要找规律找出循环条件：$columns \gt startX * 2$并且$rows \gt startY*2$。 12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding:utf-8 -*-class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): # write code here if matrix is None: return res = [] start = 0 column = len(matrix[0]) row = len(matrix) while column &gt; 2*start and row &gt; start*2: self.PrintMatrixIncicle(matrix,column,row,start,res) start += 1 return res def PrintMatrixIncicle(self,matrix,column,row,start,res): endX = column-1-start endY = row-1-start i = start while i &lt;= endX: number = matrix[start][i] self.printNumber(number,res) i += 1 if start &lt; endY: i = start+1 while i &lt;= endY: number= matrix[i][endX] self.printNumber(number,res) i += 1 if start &lt; endX and start &lt; endY: i = endX-1 while i &gt;= start: number = matrix[endY][i] self.printNumber(number,res) i -= 1 if start &lt; endX and start &lt; endY-1: i = endY-1 while i &gt;= start+1: number = matrix[i][start] self.printNumber(number,res) i -= 1 def printNumber(self,number,res): res.append(number)]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Line实习面]]></title>
    <url>%2Fpassages%2F2019-07-01-Line%E5%AE%9E%E4%B9%A0%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1. 笔试问题1：平衡二叉树的性质（笔者菜，画蛇添足了…） 正确解答：平衡二叉树或者是棵空树，或者是具有下列性质的二叉树： 它的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值不超过1。 若将二叉树节点的平衡因子（Balance Factor）定义为该节点的左子树的深度减去它的右子树的深度，则平衡二叉树上所有节点的平衡因子只可能为-1，0，1。 只要二叉树上有一个节点的平衡因子的绝对值大于1，那么这颗平衡二叉树就失去平衡了 问题2：在浏览器中输入 www.xxxx.com 后执行的全部过程。 正确解答： 客户端浏览器通过DNS解析到 www.xxxx.com 的IP地址，然后通过TCP进行封装数据包，输入到网络层。 在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 客户端的网络层不必关心应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，就是通过查找路由表决定通过哪个路径到达服务器。 客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换IP数据包。现在就可以传输了，然后发送IP数据包到达服务器的地址。 问题3： 设计实现HashTable类。(这题博主见过两次了，都没答对，是真的菜…) HashTable有两个问题需要解决，首先是key值哈希化，我们可以借助Python自带的hash函数解决key的哈希编码问题。其次是Hash 冲突的解决机制，在这里只考虑最简单的一种，即将同一个 hash 值下的不同的 key 存放在数组的同一个位置，以链表形式保存。 123456789101112131415161718192021222324252627282930class MyDict(object): def __init__(self,size = 10000): self.hash_list = [list() for _ in range(size)] self.size = size def __setitem__(self,key,value): # 利用python自带的hash函数，对key哈希并对size取模 # hashed_key位置没有值就追加，否则覆盖 hashed_key = hash(key)%self.size for item in self.hash_list[hashed_key]: if item[0] == key: item[1] = value break else: self.hash_list[hashed_key].append([key,value]) def __getitem__(self,key): # return: key所对应的value # 没有key，就抛出keyError for item in self.hash_list[hash(key)%self.size]: if item[0] == key: return item[1] raise keyError(key) def __repr__(self): # hashtable打印 result = [] for sub_list in self.hash_list: if not sub_list: continue for item in sub_list: result.append(str(item[0])+&quot;:&quot;+str(item[1])) return &quot;&#123;&quot;+&quot;,&quot;.join(result)+&quot;&#125;&quot; 参考答案：不用 Python 自带的 Dict 实现自己的 HashTable 2. 面试主要问博主与简历相关的工作与能力，后续博主会陆续更新自己的研究内容。（博主过了，但不能实习可惜…）]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce原理与排序应用]]></title>
    <url>%2Fpassages%2F2019-07-01-MapReduce%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%92%E5%BA%8F%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. MapReduce工作机制MapReduce执行总流程 MapReduce Framework如上图所示。 JobTracker: 初始化作业，分配作业，与TaskManager通信，协调整个作业的执行 TaskTracker: 保持与JobTracker的通信，执行map或者reduce任务 HDFS；保存作业的数据，配置信息等，保存作业结果 具体相关流程提交作业客户端编写完程序代码后，打成jar，然后通过相关命令向集群提交自己想要跑的MR任务，具体过程如下： 通过调用JobTracker的getNewJobId()获取当前作业id 检查作业相关路径 计算作业的输入划分，并将划分信息写到Job.split文件中 将运行作业所需要的资源包括作业jar包，配置文件和打算所得的输入划分，复制到作业对应的HDFS上 调用JobTracker的summitJob()提交，告诉JobTracker作业准备执行 初始化作业 从HDFS中读取作业对应的job.split，得到输入数据的划分信息 创建并且初始化Map任务和Reduce任务：为每个map/reduce task生成一个TaskInProgress去监控和调度该task。例如创建两个初始化task，一个初始化Map，一个初始化Reduce 分配任务JobTracker会将任务分配到TaskTracker去执行，但是怎么判断哪些TaskTracker，怎么分配任务呢？所以，我们要实现JobTracker和TaskTracker中的通信，也就是TaskTracker循环向JobTracker发送心跳，向上级报告自己这边是不是还活着，活干的怎么样了，可以接些新活等。作为JobTracker，接收到心跳信息，如果有待分配任务，就会给这个TaskTracker分配一个任务，然后taskTracker就把这个任务加入到他的任务队列中。我们可以主要看看TaskTracker中的transmitHeartBeart()和JobTracker的heartbeat()方法。 执行任务TaskTracker申请到任务后，在本地执行，主要有以下几个步骤来完成本地的步骤化: 将job.split复制到本地 将job.jar复制到本地 将job的配置信息写入到Job.xml 创建本地任务目录，解压job.rar 调用launchTaskForJob()方法发布任务 发布任务后，TaskRunner会启动新的java虚拟机来运行每个任务，以map任务为例，流程如下： 配置任务执行参数（获取java程序的执行环境和配置参数等） 在child临时文件表中添加Map任务信息 配置log文件夹，配置Map任务的执行环境和配置参数； 根据input split,生成RecordReader读取数据 为Map任务生成MapRunnable，一次从RecordReader中接收数据，并调用map函数进行处理 将Map函数的输出调用collect收集到MapOUtputBuffer中 2. MapReduce中排序应用实现过程Map阶段Read(读取) ==&gt; Collect(生成Key-Value) ==&gt; Spill(溢写) Read:从HDFS读取inputSplit（由InputFormat根据文件生成） Collect:分为map过程和partition过程，map根据inputSplit生成Key-Value对，而Partition添加分区标记（辅助排序用），并写入环形缓存区。 Spill:分为sort过程、compress过程以及combine过程。数据不断的写入环形缓存区，达到阈值之后开始溢写，在溢写的过程中进行一次Sort，这里使用的排序是快排（QuickSort）；一次溢出生成一个file，并且在生成file的过程中进行压缩（compress）；多个file又会进行一次文件合并，在文件合并的过程中进行排序，这里使用的排序是归并排序（MergeSort）。 Shuffle阶段Shuffle阶段主要就是一个数据拷贝的过程，Map端合成的大文件之后，通过HTTP服务(jetty server)拷贝到Reduce端。拷贝到Reduce端的数据并不是马上写入文件，而是同样放在缓存中，达到阈值则进行溢写。 Reduce阶段合并溢写生成的file，这里使用的排序为归并排序(MegerSort)，生成一些更大的文件(进一步减少文件个数)。在归并之后留下少量的大文件，最后对大文件进行一次最终合并，合并成一个有序的大文件(只有一个)，这里使用的排序算法为堆排序(HeapSort)。 总结综上所述，一个MapReduce过程涉及到了一次快排、两次归并以及一次堆排的操作。 参考资料Hadoop入门第三篇-MapReduce试手以及MR工作机制]]></content>
      <tags>
        <tag>分布式</tag>
        <tag>方法优化</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神策数据实习]]></title>
    <url>%2Fpassages%2F2019-07-01-%E7%A5%9E%E7%AD%96%E6%95%B0%E6%8D%AE%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1. 笔试拓扑排序：可以实现有向图以及无向图判断是否有环存在。 稳定排序算法：归并排序、基数排序、冒泡排序和直接插入排序 2. 面试时间很短，每人只有15分钟，本人大概面试了20分钟左右，面试官不是做算法的，技术能力很强（认识…），效果不理想，主要是自己菜—&gt;. 问题1：描述一下过拟合。 问题2：给一些数据，如何选取模型去挑选数据，判断与随机取数据的好坏。 问题3：分布式了解吗？MR工作机制？ 问题4：给文章数据让你统计词频，你怎么实现会有哪些问题？大量数据怎么处理？正常字典法请手撕代码。 问题5：针对于分词那么换行问题“hell-\nO world”如何处理，文章应当一部分一部分去处理，如果分句针对较长的句子该怎么办？（面试官不看好replace(“-\n”,””)）我也没有合适的解决方案…..]]></content>
      <categories>
        <category>个人笔经面经</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典笔试——找到数组第k大或第k小的数]]></title>
    <url>%2Fpassages%2F2019-06-30-%E6%89%BE%E5%88%B0%E6%95%B0%E7%BB%84%E7%AC%ACk%E5%A4%A7%E6%88%96%E7%AC%ACk%E5%B0%8F%E7%9A%84%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. 问题在未排序的数组中找到第k个最大的元素，找到数组排序后的第k个最大的元素。 示例： 输入： 1[3,2,3,1,2,4,5,5,6] 和 k=4 输出：4 2. 解题思路类快速排序思想，找到数组中元素的位置，当分界点的索引为k-1的时候，它就是第k大元素，第k小的数只需找（组数长度+1-k）大的数即可。其时间复杂度应小于$O(n\log_2n)$。 123456789101112131415161718192021class Solution(object): def findKthLargest(self,nums,k): return self.findKthSmallest(nums,len(nums)+1-k) def findKthSmallest(self,nums,k): if nums: pos = self.partition(nums,0,len(nums)-1) if k&gt;pos+1: return self.findKthSmallest(nums[pos+1:],k-pos-1) elif k&lt;pos+1: return self.findKthSmallest(nums[:pos],k) else: return nums[pos] def partition(self,nums,l,r): low = l while l&lt;r: if nums[l] &lt; nums[r]: nums[l],nums[low] = nums[low],num[l] low += 1 l += 1 nums[low],nums[r] = nums[r],nums[low] return low 3. 其他解法堆排序方法]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>python</tag>
        <tag>必会题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(SVM)原理与推导]]></title>
    <url>%2Fpassages%2F2019-06-30-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM-%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[1. 支持向量机的原理Support Vector Machine (SVM)是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分隔超平面的线形分类器。（间隔最大是它有别于感知机），通过该超平面实现对未知样本集的分类。 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机； 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机； 当训练数据线性不可分时，通过使用核技巧及间隔最大化，学习非线性支持向量机。 2. SVM推导线性分类器是最简单的有效分类器形式，SVM则在线性分类器基础上演化而成。 所谓线性函数，在一维空间中就是一个点，在二维空间中就是一条直线，三维空间就是一个平面，随着维度不断增加，线性函数也不断改变，被叫做超平面。 SVM的目标就是找到图中的超平面，在任意维空间中，超平面可以表示为函数：$$G(x)=W^TX+b$$一个样本点到$P(X_i,Y_i)$超平面的几何距离为：$$\frac{\left|W^TX_i+b\right|}{\left|{W}\right|}$$Note:这里的几何距离是距离而非间隔。 对于正样本$Y_i=1$，意味着该点在超平面正样本一侧：$$W^TX_i+b&gt;0$$对于负样本$Y_i =-1$，意味着该点在超平面负样本一侧：$$W^TX_i+b&lt;0$$所以可用下述公式表示点到超平面的几何距离：$$\frac{Y_i(W^TX_i+b)}{\left|W\right|}$$其中，分子部分表示函数间隔。 几何间隔可以表示点到超平面的距离，在进行分类过程中使分类更加准确，需要优化所有样本中到分类超平面几何间隔最小点的间隔值更大，所以对于SVM公式的求解转换为了一个最优化问题：$$\max_{W,b}[\min_{X_i}\frac{Y_i(W^TX_i+b)}{\left|W\right|}]$$即求解出最优的$W,b$使得到超平面最近的样本几何间隔最大。 对于一个超平面，如果等比例缩放$W,b$的值其公式表示的超平面不变，我们可以假设到超平面距离最小的样本点到超平面的函数间隔都大于1（通过缩放可$W,b$以实现）：$$Y_i(W^TX_i+b)\ge1$$由此可以将原最优化问题进行改变为带约束条件的最优化问题，优化目标为：$$\max_{W,b}\frac{1}{\left|W\right|}$$约束条件为：$$Y_i(W^TX_i+b)\ge1，i=1,2,3…k$$为了便于求解，目标函数等价于：$$\min_{W,b}\frac{1}{2}\left|w\right|^2$$下面便对该式子进行求解。对于没有约束的最优化问题，我们可以采用求导的方式求解出最优解。针对带约束最优化问题，一个比较好的方法是将其转化为没有约束条件的最优化问题，我们将带约束最优化问题泛化为：$$\min_xf(x)\\s.t.\\g(x)\le0,i=1,2,3…k\\h(x)=0,i=1,2,3…l$$泛化问题可以等价于求：$$\min_{x}\max_{\alpha,\beta}f(x)+\sum_{i=1}^k\alpha g_i(x)+\sum_{j=1}^l\beta h_j(x)$$其中$\alpha$大于零，$\beta$不等于零。若$g(x)$大于零，而$\alpha$也大于零，对于可变$\alpha$项则不存在极大值，关于$\beta$同理，其可正可负不为0，若$h(x)$不能满足等于零，在该项上也不存在极大值，通过max求极值实现了对原有约束条件的保留。 基于此SVM最优化方程转化为：$$\min_{W,b}\max_{\alpha}\frac{1}{2}\left|w\right|^2+\sum_{i=1}^k\alpha_{i}(1-Y_i(W^TX_i+b))\\s.t.\\\alpha_i\gt0,i=1,2,3…k$$在满足KTT条件的情况下满足拉格朗日对偶性，其等价于优化方程：$$\max_{\alpha}\min_{W,b}\frac{1}{2}\left|w\right|^2+\sum_{i=1}^k\alpha_{i}(1-Y_i(W^TX_i+b))\\s.t.\\\alpha_i\gt0,i=1,2,3…k\\1-Y_i(W^TX_i+b)\le0,i=1,2,3…k\\\alpha_{i}(1-Y_i(W^TX_i+b))=0,i=1,2,3…k$$对于该最优化的方程内部进行求导：$$\frac{\partial L}{\partial W} =W-\sum_{i=1}^k\alpha_{i}Y_iX_i\\\frac{\partial L}{\partial b} =-\sum_{i=1}^k\alpha_{i}Y_i$$在偏导数为0时取得极值。$$W= \sum_{i=1}^k\alpha_{i}Y_iX_i\\\sum_{i=1}^k\alpha_{i}Y_i=0$$将求导后的等式带入原方程可以得到最终关于$\alpha$的最优化方程：$$\max_{\alpha}\sum_{i=1}^k\alpha_i-\frac{1}{2}\sum_{i=1}^k\alpha_{i}Y_iX_i^T\sum_{j=1}^k\alpha_jY_jX_j\\s.t.\\\alpha_i\gt0,i=1,2,3…k\\1-Y_i(W^TX_i+b)\le0,i=1,2,3…k\\\alpha_{i}(1-Y_i(W^TX_i+b))=0,i=1,2,3…k\\\sum_{i=1}^k\alpha_iY_i=0$$求出该式子中各个$\alpha$的值，由于$W,b$与$\alpha$均有等价关系，便可以得到SVM模型的$W,b$参数。上面式子显然无法通过求导得到最优的$\alpha$值，可以使用Sequential Minimal Optimization(SMO)方法去进行求解。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2Fpassages%2F2019-06-30-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序1. 介绍快速排序是一种高效的排序算法，它采用“分而治之”的思想。其原理是：对于一组给定的记录，通过一趟排序后，将原序列分成两部分，其中前部分的所有记录均比后面部分的所有记录小，然后再依次对前后两部分的记录进行快速排序，递归该过程，直到序列中的所有记录均有序为止。 具体算法步骤如下： (1) 分解: 将输入的序列array[m,…,n]划分成两个非空子序列array[m,…,k]和array[k+1,…,n]，使array[m,..,k]和array[k+1,…,n]，使array[m,…,k]中任一元素的值不大于array[k+1,…,n]中任一元素的值。 (2) 递归求解：通过递归调用快速排序算法分别对array[m,…,k]和array[k+1,…,n]进行排序。 (3) 合并: 由于对分解出的两个子序列的排序是就地进行的，所以在array[m,…k]和array[k+1,…,n]都是排好序后，不需要执行任何计算，array[m,…,n]就已排好序。 2. Python代码实现123456789101112131415161718192021222324252627282930# 如何进行快速排序# 整体有序,基准左边或右边样本为空，比较次数多，最坏时间复杂度，例如基准最小，排序递增# 最坏时间复杂度为O(n^2),最好时间复杂度为O(nlogn),平均时间复杂度为O(nlogn)# 平均空间复杂度为O(nlogn)def quick_sort(left,right,lists): if left &gt; right: return lists key = lists[left] low = left high = right while left &lt; right: while left &lt;right and lists[right]&gt;=key: right -= 1 lists[left] = lists[right] while left &lt;right and lists[left] &lt;= key: left += 1 lists[right] = lists[left] lists[left] = key quick_sort(low,left-1,lists) quick_sort(left+1,high,lists) return listsif __name__ == "__main__": lists = [3,4,2,8,9,5,1] print("排序前序列为：",end="") for i in lists: print(i,end=" ") print("\n排序后序列为：",end="") for i in (quick_sort(0,len(lists)-1,lists)): print(i,end=" ") 3 参考资料 《Python程序员面试算法宝典》]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>排序算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Long and Short Term Memory（LSTM）的理解]]></title>
    <url>%2Fpassages%2F2019-06-29-Long-and-Short-Term-Memory%EF%BC%88LSTM%EF%BC%89%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. LSTM网络定义长短期记忆网络——通常被称为 LSTM，是一种特殊的 RNN，能够学习长期依赖性。由 Hochreiter 和 Schmidhuber（1997）提出的，并且在接下来的工作中被许多人改进和推广。LSTM 在各种各样的问题上表现非常出色，现在被广泛使用。 LSTM 被明确设计用来避免长期依赖性问题。长时间记住信息实际上是 LSTM 的默认行为，而不是需要努力学习的东西！ 所有递归神经网络都具有神经网络的链式重复模块。在标准的 RNN 中，这个重复模块具有非常简单的结构，只有单个 tanh 层。 LSTM 也具有这种类似的链式结构，但重复模块具有不同的结构。不是一个单独的神经网络层，而是四个，并且以非常特殊的方式进行交互。 2. LSTM的核心思想LSTM 的关键是细胞状态（cell state），即图中上方的水平线。它贯穿整个链条，只有一些次要的线性交互作用。信息很容易以不变的方式流过。LSTM可以通过“门”结构实现向细胞状态添加或移除信息的操作。 “门”结构通过一个 sigmoid 的神经层和一个逐点相乘的操作来实现信息选择通过。LSTM通过三个这样的门结构来实现信息的保护和控制。这三个门分别输入门（input gate）、遗忘门（forget gate）和输出门（output gate）。 2.1 遗忘门LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息，这一过程由遗忘门来完成，其过程如下图所示。 遗忘门会读取$h_{t-1}$和$x_t$，输出一个在 0到 1之间的数值给每个在细胞状态$C_{t-1}$中的数字。1表示“完全保留”，0 表示“完全舍弃”。 对于语言模型，若已知先前的词汇预测下一词汇，当前一刻细胞状态包括当前主语的性别，这信息需要保留下来帮助我们使用正确的代词。但当我们看到一个新的主语时，我们需要忘记先前的主语。 2.2 输入门下一步是决定让多少新的信息加入到细胞状态中来。实现这个需要包括两个步骤： 一个“输入门”的 sigmoid网络层确定哪些信息需要更新 一个 tanh 网络层创建一个新的备选值向量—— $ \hat C_t $可以用来添加到细胞状态 之后，我们把这两部分联合起来，对细胞状态进行一个更新，其过程如下图。 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。 2.3 输出门现在更新旧的细胞状态 $C_{t-1}$ 更新到 $C_t$。然后，我们对旧的状态乘 $f_t$，用来忘记我们决定忘记的事。然后我们加上 $i_t*\hat C_t$，这是新的候选值，根据我们对每个状态决定的更新值按比例进行缩放。 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪些部分将输出出去。然后，我们把细胞状态输入 $tanh$处理（把数值调整到 $−1$和 $1$ 之间）再和 sigmoid 网络层的输出相乘，这样我们就可以输出想要输出的部分。 同样，以语言模型为例子，一旦出现一个主语，主语的信息会影响到随后出现的动词。例如，知道主语是单数还是复数，就可以知道随后动词的形式。 参考资料 Stacked Long Short-Term Memory Networks Understanding LSTM Networks]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>神经网络</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法比较]]></title>
    <url>%2Fpassages%2F2019-06-29-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[各种排序算法比较 类别 排序方法 时间复杂度 空间复杂度 稳定性 平均情况 最好情况 最坏情况 辅助空间 插入排序 直接插入 O($n^2$) O($n$) O($n^2$) O($1$) 稳定 Shell排序 O($n^{1.3}$) O($n$) O($n^2$) O($1$) 不稳定 选择排序 直接选择 O($n^2$) O($n^2$) O($n^2$) O($1$) 不稳定 堆排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($1$) 不稳定 交换排序 冒泡排序 O($n^2$) O($n$) O($n^2$) O($1$) 稳定 快速排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n^2$) O($n\log_{2}{n}$) 不稳定 归并排序 O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n\log_{2}{n}$) O($n$) 稳定 基数排序 O($d(r+n)$) O($d(n+rd)$) O($d(r+n)$) O($rd+n$) 稳定 Note:基数排序的复杂度中，r代表关键字的基数，d代表长度，n代表关键字的个数。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>笔试面试</tag>
        <tag>排序算法</tag>
        <tag>复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo与github搭建个人博客]]></title>
    <url>%2Fpassages%2Fhexo%2F</url>
    <content type="text"><![CDATA[GitHub个人博客搭建主要有两种方法：一是基于Ruby的jekyll+github方法，二是基于Nodejs的Hexo+github方法。这是本人的第一篇个人博客，希望能够坚持写下去。下面我将介绍下Hexo+github方法的具体过程： 搭建过程Hexo安装12$ npm install hexo-cli -g$ npm install hexo-deployer-git --save 第一句是安装Hexo，第二句是安装Hexo部署到git page的deployer保证与github关联。 主题建站123$ cd your_blog_dir$ hexo init blog$ git clone https://github.com/dongyuanxin/theme-ad.git themes/ad 安装完成后，根据自己喜好建立目录。Hexo 将会在指定文件夹中新建所需要的文件。之后我们可以在Hexo官网上选取主题从Github上clone到本地的themes文件夹下。 针对于不同的主题，blog下的_config.yml需进行如下操作更换主题、与Github关联： 12345theme: 主题名deploy: type: git repository: git@github.com:nijunssdut/nijunssdut.github.io.git branch: master 在这里，我们配置Github的SSH密钥可以让本地git项目与远程的github建立联系，让我们在本地写了代码之后直接通过git操作就可以实现本地代码库与Github代码库同步。 12345$ cd ~/ .ssh$ ssh-keygen -t rsa -C "your_email@example.com"# 这将按照你提供的邮箱地址，创建一对密钥$ pbcopy &lt; ~/.ssh/id_rsa.pub# 将公钥的内容复制到系统粘贴板 之后在Github的Account Settings-SSH Keys中粘贴添加密钥即可。 更多信息详见: 参考博客 配置与测试这部分主要介绍Hexo命令的使用。 1234$ hexo clean$ hexo generate$ hexo server$ hexo deploy hexo clean与hexo generate一般一起使用清理并生成编写内容，执行完hexo server后可在本地使用https://localhost:4000 查看建站情况，hexo deploy会更新Github端个人博客的内容：Jun的个人主页 Markdown写作本人使用Mac OS，推荐Typora软件进行Markdown编写。Typora是一款轻便简洁的Markdown编辑器，支持即时渲染技术。 Typora语法相对简单，可参考简书typora、博客园typora。 Hexo博客内容图片显示问题12$ npm install eslint$ npm install hexo-asset-image --save 由于缺少eslint依赖直接安装Hexo图片插件会有警告。完成安装后用hexo新建文章的时候会发现_posts目录下面会多出一个和文章名字一样的文件夹。图片就可以放在文件夹下面。插入图片的方式采用Markdown语法即可。 Note:把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
